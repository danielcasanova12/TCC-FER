{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "CoaT architecture.\n",
    "\n",
    "Paper: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399\n",
    "\n",
    "Official CoaT code at: https://github.com/mlpc-ucsd/CoaT\n",
    "\n",
    "Modified from timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, LayerNorm\n",
    "def build_model_with_cfg(model_class, variant, pretrained, pretrained_filter_fn=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Build a model using the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        model_class (class): The class of the model to instantiate.\n",
    "        variant (str): The variant name of the model.\n",
    "        pretrained (bool): Whether to load pretrained weights.\n",
    "        pretrained_filter_fn (function): A function to filter the pretrained state_dict.\n",
    "        **kwargs: Additional keyword arguments for the model class.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: An instance of the model.\n",
    "    \"\"\"\n",
    "    model = model_class(**kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        # Load pretrained weights if available\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            kwargs.get('url', ''), map_location='cpu', check_hash=True\n",
    "        )\n",
    "        if pretrained_filter_fn:\n",
    "            state_dict = pretrained_filter_fn(state_dict, model)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model\n",
    "_model_registry = {}\n",
    "\n",
    "def register_model(fn):\n",
    "    \"\"\"\n",
    "    Register a model creation function.\n",
    "\n",
    "    Args:\n",
    "        fn (function): The function that creates and returns the model.\n",
    "\n",
    "    Returns:\n",
    "        function: The registered function.\n",
    "    \"\"\"\n",
    "    model_name = fn.__name__\n",
    "    _model_registry[model_name] = fn\n",
    "    return fn\n",
    "\n",
    "def get_model(name, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Retrieve a registered model by name.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the model.\n",
    "        *args, **kwargs: Additional arguments for the model creation function.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: An instance of the requested model.\n",
    "    \"\"\"\n",
    "    if name not in _model_registry:\n",
    "        raise ValueError(f\"Model '{name}' is not registered.\")\n",
    "    return _model_registry[name](*args, **kwargs)\n",
    "\n",
    "__all__ = ['CoaT']\n",
    "\n",
    "def generate_default_cfgs(model_cfg_dict):\n",
    "    \"\"\"\n",
    "    Generate default configuration for models.\n",
    "\n",
    "    Args:\n",
    "        model_cfg_dict (dict): Dictionary of model names and their configurations.\n",
    "\n",
    "    Returns:\n",
    "        dict: Default configuration for each model.\n",
    "    \"\"\"\n",
    "    default_cfgs = {}\n",
    "    for model_name, cfg in model_cfg_dict.items():\n",
    "        default_cfgs[model_name] = {\n",
    "            'mean': IMAGENET_DEFAULT_MEAN,\n",
    "            'std': IMAGENET_DEFAULT_STD,\n",
    "            'input_size': (3, 224, 224),\n",
    "            'num_classes': 1000,\n",
    "            'interpolation': 'bicubic',\n",
    "            **cfg\n",
    "        }\n",
    "    return default_cfgs\n",
    "class ConvRelPosEnc(nn.Module):\n",
    "    \"\"\" Convolutional relative position encoding. \"\"\"\n",
    "    def __init__(self, head_chs, num_heads, window):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "            Ch: Channels per head.\n",
    "            h: Number of heads.\n",
    "            window: Window size(s) in convolutional relative positional encoding. It can have two forms:\n",
    "                1. An integer of window size, which assigns all attention heads with the same window s\n",
    "                    size in ConvRelPosEnc.\n",
    "                2. A dict mapping window size to #attention head splits (\n",
    "                    e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})\n",
    "                    It will apply different window size to the attention head splits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(window, int):\n",
    "            # Set the same window size for all attention heads.\n",
    "            window = {window: num_heads}\n",
    "            self.window = window\n",
    "        elif isinstance(window, dict):\n",
    "            self.window = window\n",
    "        else:\n",
    "            raise ValueError()            \n",
    "        \n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.head_splits = []\n",
    "        for cur_window, cur_head_split in window.items():\n",
    "            dilation = 1\n",
    "            # Determine padding size.\n",
    "            # Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n",
    "            padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2\n",
    "            cur_conv = nn.Conv2d(\n",
    "                cur_head_split * head_chs,\n",
    "                cur_head_split * head_chs,\n",
    "                kernel_size=(cur_window, cur_window), \n",
    "                padding=(padding_size, padding_size),\n",
    "                dilation=(dilation, dilation),                          \n",
    "                groups=cur_head_split * head_chs,\n",
    "            )\n",
    "            self.conv_list.append(cur_conv)\n",
    "            self.head_splits.append(cur_head_split)\n",
    "        self.channel_splits = [x * head_chs for x in self.head_splits]\n",
    "\n",
    "    def forward(self, q, v, size: Tuple[int, int]):\n",
    "        B, num_heads, N, C = q.shape\n",
    "        H, W = size\n",
    "        _assert(N == 1 + H * W, '')\n",
    "\n",
    "        # Convolutional relative position encoding.\n",
    "        q_img = q[:, :, 1:, :]  # [B, h, H*W, Ch]\n",
    "        v_img = v[:, :, 1:, :]  # [B, h, H*W, Ch]\n",
    "\n",
    "        v_img = v_img.transpose(-1, -2).reshape(B, num_heads * C, H, W)\n",
    "        v_img_list = torch.split(v_img, self.channel_splits, dim=1)  # Split according to channels\n",
    "        conv_v_img_list = []\n",
    "        for i, conv in enumerate(self.conv_list):\n",
    "            conv_v_img_list.append(conv(v_img_list[i]))\n",
    "        conv_v_img = torch.cat(conv_v_img_list, dim=1)\n",
    "        conv_v_img = conv_v_img.reshape(B, num_heads, C, H * W).transpose(-1, -2)\n",
    "\n",
    "        EV_hat = q_img * conv_v_img\n",
    "        EV_hat = F.pad(EV_hat, (0, 0, 1, 0, 0, 0))  # [B, h, N, Ch].\n",
    "        return EV_hat\n",
    "\n",
    "\n",
    "class FactorAttnConvRelPosEnc(nn.Module):\n",
    "    \"\"\" Factorized attention with convolutional relative position encoding class. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            shared_crpe=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)  # Note: attn_drop is actually not used.\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # Shared convolutional relative position encoding.\n",
    "        self.crpe = shared_crpe\n",
    "\n",
    "    def forward(self, x, size: Tuple[int, int]):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Generate Q, K, V.\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)  # [B, h, N, Ch]\n",
    "\n",
    "        # Factorized attention.\n",
    "        k_softmax = k.softmax(dim=2)\n",
    "        factor_att = k_softmax.transpose(-1, -2) @ v\n",
    "        factor_att = q @ factor_att\n",
    "\n",
    "        # Convolutional relative position encoding.\n",
    "        crpe = self.crpe(q, v, size=size)  # [B, h, N, Ch]\n",
    "\n",
    "        # Merge and reshape.\n",
    "        x = self.scale * factor_att + crpe\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)  # [B, h, N, Ch] -> [B, N, h, Ch] -> [B, N, C]\n",
    "\n",
    "        # Output projection.\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvPosEnc(nn.Module):\n",
    "    \"\"\" Convolutional Position Encoding. \n",
    "        Note: This module is similar to the conditional position encoding in CPVT.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, k=3):\n",
    "        super(ConvPosEnc, self).__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim) \n",
    "    \n",
    "    def forward(self, x, size: Tuple[int, int]):\n",
    "        B, N, C = x.shape\n",
    "        H, W = size\n",
    "        _assert(N == 1 + H * W, '')\n",
    "\n",
    "        # Extract CLS token and image tokens.\n",
    "        cls_token, img_tokens = x[:, :1], x[:, 1:]  # [B, 1, C], [B, H*W, C]\n",
    "        \n",
    "        # Depthwise convolution.\n",
    "        feat = img_tokens.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.proj(feat) + feat\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Combine with CLS token.\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SerialBlock(nn.Module):\n",
    "    \"\"\" Serial block class.\n",
    "        Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            shared_cpe=None,\n",
    "            shared_crpe=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv-Attention.\n",
    "        self.cpe = shared_cpe\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.factoratt_crpe = FactorAttnConvRelPosEnc(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            shared_crpe=shared_crpe,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        # MLP.\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, size: Tuple[int, int]):\n",
    "        # Conv-Attention.\n",
    "        x = self.cpe(x, size)\n",
    "        cur = self.norm1(x)\n",
    "        cur = self.factoratt_crpe(cur, size)\n",
    "        x = x + self.drop_path(cur) \n",
    "\n",
    "        # MLP. \n",
    "        cur = self.norm2(x)\n",
    "        cur = self.mlp(cur)\n",
    "        x = x + self.drop_path(cur)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelBlock(nn.Module):\n",
    "    \"\"\" Parallel block class. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dims,\n",
    "            num_heads,\n",
    "            mlp_ratios=[],\n",
    "            qkv_bias=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            shared_crpes=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv-Attention.\n",
    "        self.norm12 = norm_layer(dims[1])\n",
    "        self.norm13 = norm_layer(dims[2])\n",
    "        self.norm14 = norm_layer(dims[3])\n",
    "        self.factoratt_crpe2 = FactorAttnConvRelPosEnc(\n",
    "            dims[1],\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            shared_crpe=shared_crpes[1],\n",
    "        )\n",
    "        self.factoratt_crpe3 = FactorAttnConvRelPosEnc(\n",
    "            dims[2],\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            shared_crpe=shared_crpes[2],\n",
    "        )\n",
    "        self.factoratt_crpe4 = FactorAttnConvRelPosEnc(\n",
    "            dims[3],\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            shared_crpe=shared_crpes[3],\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        # MLP.\n",
    "        self.norm22 = norm_layer(dims[1])\n",
    "        self.norm23 = norm_layer(dims[2])\n",
    "        self.norm24 = norm_layer(dims[3])\n",
    "        # In parallel block, we assume dimensions are the same and share the linear transformation.\n",
    "        assert dims[1] == dims[2] == dims[3]\n",
    "        assert mlp_ratios[1] == mlp_ratios[2] == mlp_ratios[3]\n",
    "        mlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n",
    "        self.mlp2 = self.mlp3 = self.mlp4 = Mlp(\n",
    "            in_features=dims[1],\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "\n",
    "    def upsample(self, x, factor: float, size: Tuple[int, int]):\n",
    "        \"\"\" Feature map up-sampling. \"\"\"\n",
    "        return self.interpolate(x, scale_factor=factor, size=size)\n",
    "\n",
    "    def downsample(self, x, factor: float, size: Tuple[int, int]):\n",
    "        \"\"\" Feature map down-sampling. \"\"\"\n",
    "        return self.interpolate(x, scale_factor=1.0/factor, size=size)\n",
    "\n",
    "    def interpolate(self, x, scale_factor: float, size: Tuple[int, int]):\n",
    "        \"\"\" Feature map interpolation. \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        H, W = size\n",
    "        _assert(N == 1 + H * W, '')\n",
    "\n",
    "        cls_token = x[:, :1, :]\n",
    "        img_tokens = x[:, 1:, :]\n",
    "        \n",
    "        img_tokens = img_tokens.transpose(1, 2).reshape(B, C, H, W)\n",
    "        img_tokens = F.interpolate(\n",
    "            img_tokens,\n",
    "            scale_factor=scale_factor,\n",
    "            recompute_scale_factor=False,\n",
    "            mode='bilinear',\n",
    "            align_corners=False,\n",
    "        )\n",
    "        img_tokens = img_tokens.reshape(B, C, -1).transpose(1, 2)\n",
    "        \n",
    "        out = torch.cat((cls_token, img_tokens), dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, sizes: List[Tuple[int, int]]):\n",
    "        _, S2, S3, S4 = sizes\n",
    "        cur2 = self.norm12(x2)\n",
    "        cur3 = self.norm13(x3)\n",
    "        cur4 = self.norm14(x4)\n",
    "        cur2 = self.factoratt_crpe2(cur2, size=S2)\n",
    "        cur3 = self.factoratt_crpe3(cur3, size=S3)\n",
    "        cur4 = self.factoratt_crpe4(cur4, size=S4)\n",
    "        upsample3_2 = self.upsample(cur3, factor=2., size=S3)\n",
    "        upsample4_3 = self.upsample(cur4, factor=2., size=S4)\n",
    "        upsample4_2 = self.upsample(cur4, factor=4., size=S4)\n",
    "        downsample2_3 = self.downsample(cur2, factor=2., size=S2)\n",
    "        downsample3_4 = self.downsample(cur3, factor=2., size=S3)\n",
    "        downsample2_4 = self.downsample(cur2, factor=4., size=S2)\n",
    "        cur2 = cur2 + upsample3_2 + upsample4_2\n",
    "        cur3 = cur3 + upsample4_3 + downsample2_3\n",
    "        cur4 = cur4 + downsample3_4 + downsample2_4\n",
    "        x2 = x2 + self.drop_path(cur2) \n",
    "        x3 = x3 + self.drop_path(cur3) \n",
    "        x4 = x4 + self.drop_path(cur4) \n",
    "\n",
    "        # MLP. \n",
    "        cur2 = self.norm22(x2)\n",
    "        cur3 = self.norm23(x3)\n",
    "        cur4 = self.norm24(x4)\n",
    "        cur2 = self.mlp2(cur2)\n",
    "        cur3 = self.mlp3(cur3)\n",
    "        cur4 = self.mlp4(cur4)\n",
    "        x2 = x2 + self.drop_path(cur2)\n",
    "        x3 = x3 + self.drop_path(cur3)\n",
    "        x4 = x4 + self.drop_path(cur4) \n",
    "\n",
    "        return x1, x2, x3, x4\n",
    "\n",
    "\n",
    "class CoaT(nn.Module):\n",
    "    \"\"\" CoaT class. \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            num_classes=1000,\n",
    "            embed_dims=(64, 128, 320, 512),\n",
    "            serial_depths=(3, 4, 6, 3),\n",
    "            parallel_depth=0,\n",
    "            num_heads=8,\n",
    "            mlp_ratios=(4, 4, 4, 4),\n",
    "            qkv_bias=True,\n",
    "            drop_rate=0.,\n",
    "            proj_drop_rate=0.,\n",
    "            attn_drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            norm_layer=LayerNorm,\n",
    "            return_interm_layers=False,\n",
    "            out_features=None,\n",
    "            crpe_window=None,\n",
    "            global_pool='token',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert global_pool in ('token', 'avg')\n",
    "        crpe_window = crpe_window or {3: 2, 5: 3, 7: 3}\n",
    "        self.return_interm_layers = return_interm_layers\n",
    "        self.out_features = out_features\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_features = self.head_hidden_size = embed_dims[-1]\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        # Patch embeddings.\n",
    "        img_size = to_2tuple(img_size)\n",
    "        self.patch_embed1 = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n",
    "            embed_dim=embed_dims[0], norm_layer=nn.LayerNorm)\n",
    "        self.patch_embed2 = PatchEmbed(\n",
    "            img_size=[x // 4 for x in img_size], patch_size=2, in_chans=embed_dims[0],\n",
    "            embed_dim=embed_dims[1], norm_layer=nn.LayerNorm)\n",
    "        self.patch_embed3 = PatchEmbed(\n",
    "            img_size=[x // 8 for x in img_size], patch_size=2, in_chans=embed_dims[1],\n",
    "            embed_dim=embed_dims[2], norm_layer=nn.LayerNorm)\n",
    "        self.patch_embed4 = PatchEmbed(\n",
    "            img_size=[x // 16 for x in img_size], patch_size=2, in_chans=embed_dims[2],\n",
    "            embed_dim=embed_dims[3], norm_layer=nn.LayerNorm)\n",
    "\n",
    "        # Class tokens.\n",
    "        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dims[0]))\n",
    "        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dims[1]))\n",
    "        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dims[2]))\n",
    "        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dims[3]))\n",
    "\n",
    "        # Convolutional position encodings.\n",
    "        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n",
    "        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n",
    "        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n",
    "        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n",
    "\n",
    "        # Convolutional relative position encodings.\n",
    "        self.crpe1 = ConvRelPosEnc(head_chs=embed_dims[0] // num_heads, num_heads=num_heads, window=crpe_window)\n",
    "        self.crpe2 = ConvRelPosEnc(head_chs=embed_dims[1] // num_heads, num_heads=num_heads, window=crpe_window)\n",
    "        self.crpe3 = ConvRelPosEnc(head_chs=embed_dims[2] // num_heads, num_heads=num_heads, window=crpe_window)\n",
    "        self.crpe4 = ConvRelPosEnc(head_chs=embed_dims[3] // num_heads, num_heads=num_heads, window=crpe_window)\n",
    "\n",
    "        # Disable stochastic depth.\n",
    "        dpr = drop_path_rate\n",
    "        assert dpr == 0.0\n",
    "        skwargs = dict(\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            proj_drop=proj_drop_rate,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "\n",
    "        # Serial blocks 1.\n",
    "        self.serial_blocks1 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[0],\n",
    "                mlp_ratio=mlp_ratios[0],\n",
    "                shared_cpe=self.cpe1,\n",
    "                shared_crpe=self.crpe1,\n",
    "                **skwargs,\n",
    "            )\n",
    "            for _ in range(serial_depths[0])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 2.\n",
    "        self.serial_blocks2 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[1],\n",
    "                mlp_ratio=mlp_ratios[1],\n",
    "                shared_cpe=self.cpe2,\n",
    "                shared_crpe=self.crpe2,\n",
    "                **skwargs,\n",
    "            )\n",
    "            for _ in range(serial_depths[1])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 3.\n",
    "        self.serial_blocks3 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[2],\n",
    "                mlp_ratio=mlp_ratios[2],\n",
    "                shared_cpe=self.cpe3,\n",
    "                shared_crpe=self.crpe3,\n",
    "                **skwargs,\n",
    "            )\n",
    "            for _ in range(serial_depths[2])]\n",
    "        )\n",
    "\n",
    "        # Serial blocks 4.\n",
    "        self.serial_blocks4 = nn.ModuleList([\n",
    "            SerialBlock(\n",
    "                dim=embed_dims[3],\n",
    "                mlp_ratio=mlp_ratios[3],\n",
    "                shared_cpe=self.cpe4,\n",
    "                shared_crpe=self.crpe4,\n",
    "                **skwargs,\n",
    "            )\n",
    "            for _ in range(serial_depths[3])]\n",
    "        )\n",
    "\n",
    "        # Parallel blocks.\n",
    "        self.parallel_depth = parallel_depth\n",
    "        if self.parallel_depth > 0:\n",
    "            self.parallel_blocks = nn.ModuleList([\n",
    "                ParallelBlock(\n",
    "                    dims=embed_dims,\n",
    "                    mlp_ratios=mlp_ratios,\n",
    "                    shared_crpes=(self.crpe1, self.crpe2, self.crpe3, self.crpe4),\n",
    "                    **skwargs,\n",
    "                )\n",
    "                for _ in range(parallel_depth)]\n",
    "            )\n",
    "        else:\n",
    "            self.parallel_blocks = None\n",
    "\n",
    "        # Classification head(s).\n",
    "        if not self.return_interm_layers:\n",
    "            if self.parallel_blocks is not None:\n",
    "                self.norm2 = norm_layer(embed_dims[1])\n",
    "                self.norm3 = norm_layer(embed_dims[2])\n",
    "            else:\n",
    "                self.norm2 = self.norm3 = None\n",
    "            self.norm4 = norm_layer(embed_dims[3])\n",
    "\n",
    "            if self.parallel_depth > 0:\n",
    "                # CoaT series: Aggregate features of last three scales for classification.\n",
    "                assert embed_dims[1] == embed_dims[2] == embed_dims[3]\n",
    "                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n",
    "                self.head_drop = nn.Dropout(drop_rate)\n",
    "                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "            else:\n",
    "                # CoaT-Lite series: Use feature of last scale for classification.\n",
    "                self.aggregate = None\n",
    "                self.head_drop = nn.Dropout(drop_rate)\n",
    "                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Initialize weights.\n",
    "        trunc_normal_(self.cls_token1, std=.02)\n",
    "        trunc_normal_(self.cls_token2, std=.02)\n",
    "        trunc_normal_(self.cls_token3, std=.02)\n",
    "        trunc_normal_(self.cls_token4, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token1', 'cls_token2', 'cls_token3', 'cls_token4'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        assert not enable, 'gradient checkpointing not supported'\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def group_matcher(self, coarse=False):\n",
    "        matcher = dict(\n",
    "            stem1=r'^cls_token1|patch_embed1|crpe1|cpe1',\n",
    "            serial_blocks1=r'^serial_blocks1\\.(\\d+)',\n",
    "            stem2=r'^cls_token2|patch_embed2|crpe2|cpe2',\n",
    "            serial_blocks2=r'^serial_blocks2\\.(\\d+)',\n",
    "            stem3=r'^cls_token3|patch_embed3|crpe3|cpe3',\n",
    "            serial_blocks3=r'^serial_blocks3\\.(\\d+)',\n",
    "            stem4=r'^cls_token4|patch_embed4|crpe4|cpe4',\n",
    "            serial_blocks4=r'^serial_blocks4\\.(\\d+)',\n",
    "            parallel_blocks=[  # FIXME (partially?) overlap parallel w/ serial blocks??\n",
    "                (r'^parallel_blocks\\.(\\d+)', None),\n",
    "                (r'^norm|aggregate', (99999,)),\n",
    "            ]\n",
    "        )\n",
    "        return matcher\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self) -> nn.Module:\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n",
    "        self.num_classes = num_classes\n",
    "        if global_pool is not None:\n",
    "            assert global_pool in ('token', 'avg')\n",
    "            self.global_pool = global_pool\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x0):\n",
    "        B = x0.shape[0]\n",
    "\n",
    "        # Serial blocks 1.\n",
    "        x1 = self.patch_embed1(x0)\n",
    "        H1, W1 = self.patch_embed1.grid_size\n",
    "        x1 = insert_cls(x1, self.cls_token1)\n",
    "        for blk in self.serial_blocks1:\n",
    "            x1 = blk(x1, size=(H1, W1))\n",
    "        x1_nocls = remove_cls(x1).reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Serial blocks 2.\n",
    "        x2 = self.patch_embed2(x1_nocls)\n",
    "        H2, W2 = self.patch_embed2.grid_size\n",
    "        x2 = insert_cls(x2, self.cls_token2)\n",
    "        for blk in self.serial_blocks2:\n",
    "            x2 = blk(x2, size=(H2, W2))\n",
    "        x2_nocls = remove_cls(x2).reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Serial blocks 3.\n",
    "        x3 = self.patch_embed3(x2_nocls)\n",
    "        H3, W3 = self.patch_embed3.grid_size\n",
    "        x3 = insert_cls(x3, self.cls_token3)\n",
    "        for blk in self.serial_blocks3:\n",
    "            x3 = blk(x3, size=(H3, W3))\n",
    "        x3_nocls = remove_cls(x3).reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Serial blocks 4.\n",
    "        x4 = self.patch_embed4(x3_nocls)\n",
    "        H4, W4 = self.patch_embed4.grid_size\n",
    "        x4 = insert_cls(x4, self.cls_token4)\n",
    "        for blk in self.serial_blocks4:\n",
    "            x4 = blk(x4, size=(H4, W4))\n",
    "        x4_nocls = remove_cls(x4).reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Only serial blocks: Early return.\n",
    "        if self.parallel_blocks is None:\n",
    "            if not torch.jit.is_scripting() and self.return_interm_layers:\n",
    "                # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n",
    "                feat_out = {}   \n",
    "                if 'x1_nocls' in self.out_features:\n",
    "                    feat_out['x1_nocls'] = x1_nocls\n",
    "                if 'x2_nocls' in self.out_features:\n",
    "                    feat_out['x2_nocls'] = x2_nocls\n",
    "                if 'x3_nocls' in self.out_features:\n",
    "                    feat_out['x3_nocls'] = x3_nocls\n",
    "                if 'x4_nocls' in self.out_features:\n",
    "                    feat_out['x4_nocls'] = x4_nocls\n",
    "                return feat_out\n",
    "            else:\n",
    "                # Return features for classification.\n",
    "                x4 = self.norm4(x4)\n",
    "                return x4\n",
    "\n",
    "        # Parallel blocks.\n",
    "        for blk in self.parallel_blocks:\n",
    "            x2, x3, x4 = self.cpe2(x2, (H2, W2)), self.cpe3(x3, (H3, W3)), self.cpe4(x4, (H4, W4))\n",
    "            x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4)])\n",
    "\n",
    "        if not torch.jit.is_scripting() and self.return_interm_layers:\n",
    "            # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n",
    "            feat_out = {}   \n",
    "            if 'x1_nocls' in self.out_features:\n",
    "                x1_nocls = remove_cls(x1).reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x1_nocls'] = x1_nocls\n",
    "            if 'x2_nocls' in self.out_features:\n",
    "                x2_nocls = remove_cls(x2).reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x2_nocls'] = x2_nocls\n",
    "            if 'x3_nocls' in self.out_features:\n",
    "                x3_nocls = remove_cls(x3).reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x3_nocls'] = x3_nocls\n",
    "            if 'x4_nocls' in self.out_features:\n",
    "                x4_nocls = remove_cls(x4).reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                feat_out['x4_nocls'] = x4_nocls\n",
    "            return feat_out\n",
    "        else:\n",
    "            x2 = self.norm2(x2)\n",
    "            x3 = self.norm3(x3)\n",
    "            x4 = self.norm4(x4)\n",
    "            return [x2, x3, x4]\n",
    "\n",
    "    def forward_head(self, x_feat: Union[torch.Tensor, List[torch.Tensor]], pre_logits: bool = False):\n",
    "        if isinstance(x_feat, list):\n",
    "            assert self.aggregate is not None\n",
    "            if self.global_pool == 'avg':\n",
    "                x = torch.cat([xl[:, 1:].mean(dim=1, keepdim=True) for xl in x_feat], dim=1)  # [B, 3, C]\n",
    "            else:\n",
    "                x = torch.stack([xl[:, 0] for xl in x_feat], dim=1)  # [B, 3, C]\n",
    "            x = self.aggregate(x).squeeze(dim=1)  # Shape: [B, C]\n",
    "        else:\n",
    "            x = x_feat[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x_feat[:, 0]\n",
    "        x = self.head_drop(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        if not torch.jit.is_scripting() and self.return_interm_layers:\n",
    "            # Return intermediate features (for down-stream tasks).\n",
    "            return self.forward_features(x)\n",
    "        else:\n",
    "            # Return features for classification.\n",
    "            x_feat = self.forward_features(x)\n",
    "            x = self.forward_head(x_feat)\n",
    "            return x\n",
    "\n",
    "\n",
    "def insert_cls(x, cls_token):\n",
    "    \"\"\" Insert CLS token. \"\"\"\n",
    "    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def remove_cls(x):\n",
    "    \"\"\" Remove CLS token. \"\"\"\n",
    "    return x[:, 1:, :]\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    out_dict = {}\n",
    "    state_dict = state_dict.get('model', state_dict)\n",
    "    for k, v in state_dict.items():\n",
    "        # original model had unused norm layers, removing them requires filtering pretrained checkpoints\n",
    "        if k.startswith('norm1') or \\\n",
    "                (k.startswith('norm2') and getattr(model, 'norm2', None) is None) or \\\n",
    "                (k.startswith('norm3') and getattr(model, 'norm3', None) is None) or \\\n",
    "                (k.startswith('norm4') and getattr(model, 'norm4', None) is None) or \\\n",
    "                (k.startswith('aggregate') and getattr(model, 'aggregate', None) is None) or \\\n",
    "                (k.startswith('head') and getattr(model, 'head', None) is None):\n",
    "            continue\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_coat(variant, pretrained=False, default_cfg=None, **kwargs):\n",
    "    if kwargs.get('features_only', None):\n",
    "        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        CoaT,\n",
    "        variant,\n",
    "        pretrained,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def _cfg_coat(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed1.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = generate_default_cfgs({\n",
    "    'coat_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_mini.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_small.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_lite_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_lite_mini.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_lite_small.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_lite_medium.in1k': _cfg_coat(hf_hub_id='timm/'),\n",
    "    'coat_lite_medium_384.in1k': _cfg_coat(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash',\n",
    "    ),\n",
    "})\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_tiny(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6)\n",
    "    model = _create_coat('coat_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_mini(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[152, 216, 216, 216], serial_depths=[2, 2, 2, 2], parallel_depth=6)\n",
    "    model = _create_coat('coat_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_small(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[152, 320, 320, 320], serial_depths=[2, 2, 2, 2], parallel_depth=6, **kwargs)\n",
    "    model = _create_coat('coat_small', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_tiny(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[64, 128, 256, 320], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])\n",
    "    model = _create_coat('coat_lite_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_mini(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])\n",
    "    model = _create_coat('coat_lite_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_small(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[3, 4, 6, 3], mlp_ratios=[8, 8, 4, 4])\n",
    "    model = _create_coat('coat_lite_small', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_medium(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])\n",
    "    model = _create_coat('coat_lite_medium', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coat_lite_medium_384(pretrained=False, **kwargs) -> CoaT:\n",
    "    model_cfg = dict(\n",
    "        img_size=384, patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])\n",
    "    model = _create_coat('coat_lite_medium_384', pretrained=pretrained, **dict(model_cfg, **kwargs))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os datasets...\n",
      "Carregando o modelo CoaT...\n",
      "Iniciando o treinamento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando o treinamento...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 114\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Estatísticas\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm  # Corrigida a importação de tqdm\n",
    "\n",
    "# Diretórios do conjunto de dados\n",
    "train_dir = \"../data/Fer-2013/treino\"\n",
    "test_dir = \"../data/Fer-2013/teste\"\n",
    "\n",
    "# Hiperparâmetros\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformações nos dados\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # CoaT espera 3 canais\n",
    "    transforms.Resize((224, 224)),  # Dimensão compatível com o CoaT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalização padrão\n",
    "])\n",
    "\n",
    "# Carregando os datasets\n",
    "print(\"Carregando os datasets...\")\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms)\n",
    "\n",
    "# Divisão em treino e validação\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Modelo CoaT\n",
    "print(\"Carregando o modelo CoaT...\")\n",
    "model = coat_tiny(pretrained=False, num_classes=len(train_dataset.dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Função de perda e otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Função de treinamento\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(train_loader, desc=\"Treinando\", leave=False)\n",
    "    \n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estatísticas\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Atualizar a barra de progresso\n",
    "        loop.set_postfix(loss=(running_loss / total), accuracy=(correct / total))\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(val_loader, desc=\"Validando\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Estatísticas\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Atualizar a barra de progresso\n",
    "            loop.set_postfix(loss=(running_loss / total), accuracy=(correct / total))\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Treinamento\n",
    "print(\"Iniciando o treinamento...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "print(\"Avaliando no conjunto de teste...\")\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
