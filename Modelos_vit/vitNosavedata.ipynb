{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "'''MLP AND LINEARS'''\n",
    "\n",
    "def init_relu(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.orthogonal_(module.weight, gain=1.41421)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_orth(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.orthogonal_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_xavier(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_uniform_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "def init_xavier_normal(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_zeros(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.zeros_(module.weight)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def init_sigmoid(module):\n",
    "    #print(f\"The init sigmoid was only tested by the package's author at the CfC.\")\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_lecun(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=1.0 / (module.weight.shape[1])**0.5)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_tanh(module):\n",
    "    #print(f\"The init tanh was only tested by the package's author at the CfC.\")\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1.6667)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_deep_lstm(module):\n",
    "    # Ref: Sequence to Sequence Learning with Neural Networks\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.uniform_(module.weight, -0.08, 0.08)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_alphastar_special(module):\n",
    "    # Ref: Alphastar\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.005)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_emb(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        torch.nn.init.normal_(module.weight, std=math.sqrt(1/module.weight.shape[0]))\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias) \n",
    "            \n",
    "    if type(module) == nn.Embedding:\n",
    "        torch.nn.init.normal_(module.weight, std=math.sqrt(1/module.weight.shape[1]))\n",
    "\n",
    "def init_saving_variance(module, num_blks):\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(module.weight, gain=torch.tensor(4*num_blks).pow(-1/4))\n",
    "    if hasattr(module, 'bias'):\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "def init_gpt(module):\n",
    "    #print(f\"From init_gpt.\\nGpt proj linears should have a special weight initialization not implemented here.\")\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        #torch.nn.init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        #torch.nn.init.xavier_normal_(module.weight)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        nn.init.constant_(module.weight, 1.0)\n",
    "        \n",
    "\n",
    "def init_proj(module):\n",
    "    assert not isinstance(module, nn.Conv1d) and not isinstance(module, nn.Conv2d) and not isinstance(module, nn.Conv3d)\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.eye_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "'''CNN'''\n",
    "\n",
    "def init_cnn(module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.Conv1d or type(module) == nn.Conv3d:\n",
    "        #nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in', nonlinearity='SiLU')\n",
    "        nn.init.orthogonal_(module.weight, 1)\n",
    "        #nn.init.orthogonal_(module.weight, 1.41421)\n",
    "        #nn.init.xavier_uniform_(module.weight, 1)\n",
    "        #nn.init.xavier_uniform_(module.weight, 1.41421)\n",
    "\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_partial_dirac(module):\n",
    "    if type(module) in (nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        w = module.weight.data\n",
    "        \n",
    "        nn.init.dirac_(module.weight[:w.shape[1]])\n",
    "        nn.init.xavier_uniform_(module.weight[w.shape[1]:], gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    if type(module) == nn.Linear:\n",
    "        print(f\"ERROR: ONLY CONVOLUTIONS ARE SUPPORTED BY THE DIRAC INITIALIZATION.\")\n",
    "\n",
    "def init_dreamer_normal(module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.Conv1d or type(module) == nn.Conv3d:\n",
    "\n",
    "        if type(module)==nn.Linear():\n",
    "            space = module.weight.shape[1] * module.weight.shape[0]\n",
    "            in_num = space * module.weight.shape[1]\n",
    "            out_num = space * module.weight.shape[1]\n",
    "        else:\n",
    "            space = module.kernel_size[0] * module.kernel_size[1]\n",
    "            in_num = space * module.in_channels\n",
    "            out_num = space * module.out_channels\n",
    "        \n",
    "        std = np.sqrt((1/np.mean(np.array([in_num, out_num])))) / 0.87962566103423978\n",
    "        nn.init.trunc_normal_(module.weight.data, mean=0.0, std=std, a=-2.0 * std, b=2.0 * std)\n",
    "        \n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "        \n",
    "\n",
    "def init_dreamer_uniform(m):\n",
    "    # Same as xavier uniform\n",
    "    '''\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        #nn.init.orthogonal_(m.weight, 1.41421)\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        in_num = m.in_features\n",
    "        out_num = m.out_features\n",
    "        denoms = (in_num + out_num) / 2.0\n",
    "        scale = 1.0 / denoms\n",
    "        limit = np.sqrt(3 * scale)\n",
    "        nn.init.uniform_(m.weight.data, a=-limit, b=limit)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "    \n",
    "\n",
    "    \n",
    "def init_proj2d(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        torch.nn.init.dirac_(module.weight, groups=1)\n",
    "        \n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "'''WHITENED LAYERS'''\n",
    "\n",
    "def get_patches(x, patch_shape):\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    \n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    n,c,h,w = patches.shape\n",
    "    patches_flat = patches.view(n, -1)\n",
    "    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
    "    \n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "def init_whitening_conv(layer, train_set, eps=5e-4):\n",
    "    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n",
    "    \n",
    "    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n",
    "    \n",
    "    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n",
    "    \n",
    "    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "    layer.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCES\n",
    "# https://github.com/karpathy/nanoGPT\n",
    "# https://github.com/JegZheng/truncated-diffusion-probabilistic-models\n",
    "# https://github.com/facebookresearch/DiT/blob/main/models.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script # JIT decorator\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "from torch import nn\n",
    "import inspect\n",
    "def network_ema(target_network, new_network, alpha=0.5):\n",
    "    for (param_name, param_target), param_new  in zip(target_network.cuda().named_parameters(), new_network.parameters()):\n",
    "        if 'ln' in param_name: #layer norm\n",
    "            param_target.data = param_new.data.clone()\n",
    "        else:\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param_new.data.clone()\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def params_count(model, name='Model'):\n",
    "    params_to_count = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f'{name} Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "\n",
    "def params_and_grad_norm(model):\n",
    "    param_norm, grad_norm = 0, 0\n",
    "    for n, param in model.named_parameters():\n",
    "        if not n.endswith('.bias'):\n",
    "            param_norm += torch.norm(param.data)\n",
    "            if param.grad is not None:\n",
    "                grad_norm += torch.norm(param.grad)\n",
    "    return param_norm, grad_norm\n",
    "\n",
    "\n",
    "# From STORM Atari-100k\n",
    "def seed_np_torch(seed=20001118):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "\n",
    "def statistical_difference(p1, p2, n):\n",
    "    # order invariant\n",
    "    \n",
    "    d=torch.tensor(p1-p2).abs()\n",
    "    std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "    difference = torch.tensor([d-std, d+std])\n",
    "        \n",
    "    difference = difference.sort()[0]\n",
    "    \n",
    "    return difference\n",
    "\n",
    "def renormalize(tensor):\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.view(shape[0], -1)\n",
    "    max_value,_ = torch.max(tensor, -1, keepdim=True)\n",
    "    min_value,_ = torch.min(tensor, -1, keepdim=True)\n",
    "    return ((tensor - min_value) / (max_value - min_value + 1e-5)).view(shape)\n",
    "\n",
    "# Hyper Parameters\n",
    "# automatically saves all arguments of the inherited class __init__\n",
    "class Hypers: # Sorcery\n",
    "    def __init__(self, max_depth=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hypers(max_depth)\n",
    "    \n",
    "    def save_hypers(self, max_depth, ignore=[]):\n",
    "      \"\"\"Save function arguments into class attributes.\"\"\"\n",
    "\n",
    "      #f_back: frame caller\n",
    "      #frame: table of local variablies to the frame's function\n",
    "      seen_init=False\n",
    "      frame = inspect.currentframe()\n",
    "      for d in range(max_depth):\n",
    "          \n",
    "          frame = frame.f_back\n",
    "          \n",
    "          if frame.f_back and frame.f_back.f_code.co_name == \"__init__\":\n",
    "              seen_init=True\n",
    "              \n",
    "          if seen_init and frame.f_back.f_code.co_name != \"__init__\":\n",
    "              break\n",
    "            \n",
    "      _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "      #takes the arguments of the function which called this save_hypers function\n",
    "      #it can backtrack functions according to the depth argument\n",
    "\n",
    "      self.hparams = {k:v for k, v in local_vars.items()\n",
    "          if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "      for k, v in self.hparams.items():\n",
    "          setattr(self, k, v)\n",
    "\n",
    "\n",
    "# ALLWAYS PUT HYPERS TO THE LEFT\n",
    "class nsd_Module(Hypers, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_depth=3)\n",
    "class FusedGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return fused_gelu(x)\n",
    "\n",
    "\n",
    "class LayerNormNoBias(nn.Module):\n",
    "    \"\"\" LayerNormNoBias but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "    \n",
    "class Attention(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1, seq_len=8):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.k_pre = None\n",
    "        self.k_post = None\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    def forward_xl(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "\n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        self.k_pre = k.detach()\n",
    "        self.v_pre = v.detach()\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "    def forward_xl_windowed(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        if self.k_pre == None:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        elif self.k_pre.shape[-2] < self.seq_len:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        else:\n",
    "            self.k_pre = k[...,1:,:].detach()\n",
    "            self.v_pre = v[...,1:,:].detach()\n",
    "\n",
    "\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.k_post,k),-2)\n",
    "            v = torch.cat((self.v_post,v),-2)\n",
    "        \n",
    "\n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class Attention_XL(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "\n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        self.k_pre = k.detach()\n",
    "        self.v_pre = v.detach()\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class Attention_XL_window(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1, seq_len=8):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        if self.k_pre == None:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        elif self.k_pre.shape[-2] < self.seq_len:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        else:\n",
    "            self.k_pre = k[...,1:,:].detach()\n",
    "            self.v_pre = v[...,1:,:].detach()\n",
    "\n",
    "\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "\n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MemoryAttention(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_kv = nn.Linear(d_model, 2 * d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, q):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k, v  = self.W_kv(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        \n",
    "        # FoT LongLlama contrastive style (data pipeline constrastive for self attention enrichment)\n",
    "        \n",
    "        shifted_k=[]\n",
    "        shifted_v=[]\n",
    "        for i in range(7): # 7 is d-1 for d=8\n",
    "            shifted_k.append(torch.roll(k[:,:T//2],i,0))\n",
    "            shifted_v.append(torch.roll(v[:,:T//2],i,0))\n",
    "        shifted_k=torch.stack(shifted_k).view(B,-1,C)\n",
    "        shifted_v=torch.stack(shifted_v).view(B,-1,C)\n",
    "        \n",
    "        k=torch.concat((shifted_k,k),1)\n",
    "        v=torch.concat((shifted_v,v),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        L = q.shape[2]\n",
    "        S = k.shape[2]\n",
    "        attn_mask = torch.ones(L, S, dtype=torch.bool, device='cuda').tril(diagonal=S-L)\n",
    "        attn_mask[:T//2,:S-L]=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "            #y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, dropout_p=self.dropout if self.training else 0)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    def forward_memory(self, x, q, k_read, v_read):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        \n",
    "        k, v = self.W_kv(x).split(self.n_embd, dim=2)\n",
    "        write_k, write_v = k.detach(), v.detach()\n",
    "        \n",
    "        k=torch.cat((k_read, k), 1)\n",
    "        v=torch.cat((v_read, v), 1)\n",
    "        \n",
    "        #shifted_k=[]\n",
    "        #shifted_v=[]\n",
    "        #for i in range(7): # 7 is d-1 for d=8\n",
    "        #    shifted_k.append(torch.roll(k[:,:T//2],i,0))\n",
    "        #    shifted_v.append(torch.roll(v[:,:T//2],i,0))\n",
    "        #shifted_k=torch.stack(shifted_k).view(B,-1,C)\n",
    "        #shifted_v=torch.stack(shifted_v).view(B,-1,C)\n",
    "        \n",
    "        #k=torch.cat((shifted_k, k), 1)\n",
    "        #v=torch.cat((shifted_v, v), 1)\n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k_read = k_read.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2)\n",
    "        v_read = v_read.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2)\n",
    "          \n",
    "        \n",
    "        # Causal Mask\n",
    "        L = q.shape[2]\n",
    "        S = k.shape[2]-q.shape[2]\n",
    "        causal_mask = torch.ones(L, L, dtype=torch.bool, device='cuda').tril(diagonal=0)\n",
    "        eye_mask=torch.eye(L, dtype=torch.bool, device='cuda')\n",
    "        read_attnmask=torch.ones(L, L*3, dtype=torch.bool, device='cuda')\n",
    "        aux=torch.arange(L).repeat_interleave(3)\n",
    "        \n",
    "        #new_attnmask=causal_mask[:,aux]\n",
    "        read_attnmask=eye_mask[:,aux]\n",
    "        \n",
    "        attn_mask=torch.cat((read_attnmask,causal_mask),1)\n",
    "        \n",
    "        #shift_mask = torch.ones(L, int(L*3.5), dtype=torch.bool, device='cuda')\n",
    "        #shift_mask[:T//2,:]=False\n",
    "        #attn_mask=torch.cat((shift_mask,attn_mask),1)\n",
    "        \n",
    "        \n",
    "        # Memory Mask\n",
    "        memory_mask = torch.ones(L*3, L, dtype=torch.bool, device='cuda')\n",
    "        memory_mask=torch.concat((torch.eye(L*3, dtype=torch.bool, device='cuda'), memory_mask),1)\n",
    "        #memory_mask=torch.concat((~torch.ones(L*3, int(L*3.5), dtype=torch.bool, device='cuda'), memory_mask),1)\n",
    "        \n",
    "        # Associative Learning\n",
    "        std=0.5\n",
    "        noise=torch.randn_like(k_read)*std\n",
    "        k_read=F.normalize(k_read)\n",
    "        k_read=k_read+noise\n",
    "        \n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask,\n",
    "                                                dropout_p=self.dropout)\n",
    "            v_read = F.scaled_dot_product_attention(k_read,k,v, attn_mask=memory_mask,\n",
    "                                                    dropout_p=0)\n",
    "            k_read = F.scaled_dot_product_attention(k_read,k,k, attn_mask=memory_mask,\n",
    "                                                    dropout_p=0)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        k_read = k_read.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        v_read = v_read.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        \n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y, write_k, write_v, k_read, v_read\n",
    "        #return y, write_k, write_v, None,None\n",
    "\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout=0.1, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.fc    = nn.Linear(d_model, ffn_mult * d_model, bias=bias)\n",
    "        self.gelu  = nn.GELU()\n",
    "        self.proj  = nn.Linear(ffn_mult * d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPT_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, seq_len=8):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout, seq_len)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward_xl_windowed(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn.forward_xl_windowed(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x    \n",
    "    \n",
    "\n",
    "\n",
    "class GPT_Transformer(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_encoding = nn.Sequential(nn.Linear(seq_len, d_model, bias=False),\n",
    "        #                                  LayerNormNoBias(d_model)) #Stable Embedding Layer # Requires One Hot\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), GPT_Block(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult, seq_len=seq_len))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "        \n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "\n",
    "    def forward_xl_windowed(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk.forward_xl_windowed(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)    \n",
    "\n",
    "\n",
    "class GPT_NLP(nsd_Module):\n",
    "    def __init__(self, hiddens, num_blks, nhead, seq_len, vocab_size=50257,\n",
    "                 temperature=1.0, k=20, p=0.9, sampling='gpt', report_params_count=True, tied_weights=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.emb_vocab = nn.Embedding(vocab_size, hiddens)\n",
    "        self.gpt = GPT_Transformer(hiddens, nhead=nhead, num_blks=num_blks)\n",
    "        \n",
    "        self.cls = nn.Linear(hiddens, vocab_size, bias=False)\n",
    "        \n",
    "        if tied_weights:\n",
    "            self.emb_vocab.weight = self.cls.weight\n",
    "\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT NLP Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "    def forward(self, X, is_causal=True):\n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        mask = X>self.vocab_size\n",
    "        X[mask] = self.vocab_size-1\n",
    "        \n",
    "        X = self.emb_vocab(X)\n",
    "        #cls = torch.autograd.Variable(torch.zeros(batch_size, 2, self.hiddens)).to('cuda')\n",
    "        \n",
    "        #X = torch.cat((X, cls), dim=1)\n",
    "        X = self.gpt(X, is_causal=is_causal)\n",
    "\n",
    "        return self.cls(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPT_Block_XL(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, seq_len=8, windowed=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        if windowed:\n",
    "            self.attn = Attention_XL_window(d_model, nhead, bias, dropout, seq_len=seq_len)\n",
    "        else:\n",
    "            self.attn = Attention_XL(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class GPT_Transformer_XL(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, windowed=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_encoding = nn.Sequential(nn.Linear(seq_len, d_model, bias=False),\n",
    "        #                                  LayerNormNoBias(d_model)) #Stable Embedding Layer # Requires One Hot\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), GPT_Block_XL(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult, seq_len=seq_len, windowed=windowed))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "        \n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer_Block_NoLN(nsd_Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, stochastic_depth=1):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        #x = renormalize(x)\n",
    "        keep_path = torch.ones(x.shape[0],device='cuda')*(self.stochastic_depth if self.training else 1)\n",
    "        keep_path = torch.bernoulli(keep_path)[:,None,None]\n",
    "\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)*keep_path\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))*keep_path\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer_NoDATA(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0, scale_init=1):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = d_model\n",
    "        self.scale_init=scale_init\n",
    "        if scale_init==1:\n",
    "            self.scale_init=num_blks\n",
    "\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "\n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "        self.num_blks=num_blks\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), Transformer_Block_NoLN(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult,\n",
    "                                stochastic_depth=1-((1-stochastic_depth)*i/num_blks) ))\n",
    "\n",
    "\n",
    "        # https://proceedings.mlr.press/v119/huang20f/huang20f.pdf\n",
    "\n",
    "        self.apply(init_gpt)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=1/math.sqrt(self.num_hiddens))\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=(torch.tensor(4*self.scale_init,dtype=torch.float)).pow(-1/4))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        \n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def no_pos(self, X, is_causal=True):\n",
    "        X = self.start_dropout(X)\n",
    "        \n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "\n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def masked(self, X, mask, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        X = X.gather(1, mask)\n",
    "        \n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "\n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def modulate(x, shift, scale):\n",
    "    # x (B, T, D)\n",
    "    # shift (B, D)\n",
    "    # scale (B, D)\n",
    "    \n",
    "    return x * (1 + scale[:,None]) + shift[:,None]\n",
    "\n",
    "\n",
    "class DiT_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "        \n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, 6 * d_model, bias=True)\n",
    "        )\n",
    "\n",
    "        self.ln_1.apply(init_gpt)\n",
    "        self.attn.apply(init_gpt)\n",
    "        self.ln_2.apply(init_gpt)\n",
    "        self.mlp.apply(init_gpt)\n",
    "        self.adaLN_modulation.apply(init_zeros)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        \n",
    "        x_ln = modulate(self.ln_1(x), shift_msa, scale_msa)\n",
    "        \n",
    "        x = x + (1+gate_msa[:,None]) * self.attn(x_ln, x_ln, x_ln, is_causal=False)\n",
    "        x = x + (1+gate_mlp[:,None]) * self.mlp(modulate(self.ln_2(x), shift_mlp, scale_mlp))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward_no_dit(self, x):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=False)\n",
    "        return x + self.mlp(self.ln_2(x))\n",
    "    \n",
    "    \n",
    "class DiT_Transformer(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, scale_init=1):\n",
    "        super().__init__()\n",
    "        if scale_init==1:\n",
    "            scale_init=num_blks\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), DiT_Block(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(init_gpt)\n",
    "        self.init_weights()\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "\n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \n",
    "        # Zero-out adaLN modulation layers in DiT blocks:\n",
    "        for block in self.blks:\n",
    "            block.adaLN_modulation[-1].apply(init_zeros)\n",
    "    \n",
    "        \n",
    "    def forward(self, X, c):\n",
    "        # Input:\n",
    "        # X e (B, T, D)\n",
    "        # c e (B, D)\n",
    "        \n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        \n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, c)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "    \n",
    "\n",
    "    def forward_no_dit(self, X):\n",
    "        # Input:\n",
    "        # X e (B, T, D)\n",
    "        # c e (B, D)\n",
    "        \n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        \n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk.forward_no_dit(X)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "class CrossAttention_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias)\n",
    "\n",
    "    def forward(self, q, k, v, is_causal=False):\n",
    "        q = q + self.attn(self.ln_1(q),self.ln_1(k),self.ln_1(v), is_causal=is_causal)\n",
    "        q = q + self.mlp(self.ln_2(q))\n",
    "        return q\n",
    "    \n",
    "\n",
    "\n",
    "class CrossAttention_Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len, dim_feedforward=2048,  \n",
    "                 dropout = 0.1, vocab_size = 0, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.out_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), CrossAttention_Block(\n",
    "                                d_model, nhead, dropout, bias=False))\n",
    "            \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "\n",
    "\n",
    "        \n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "    \n",
    "    def forward(self, q, k, v, is_causal=False):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        q = self.start_dropout(q+pos_emb)\n",
    "        k = self.start_dropout(k+pos_emb)\n",
    "        v = self.start_dropout(v+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            q = blk.forward(q,k,v, is_causal)\n",
    "        q = self.out_ln(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SpatialNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.\n",
    "\n",
    "    Args:\n",
    "        f_channels (`int`):\n",
    "            The number of channels for input to group normalization layer, and output of the spatial norm layer.\n",
    "        zq_channels (`int`):\n",
    "            The number of channels for the quantized vector as described in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f_channels: int,\n",
    "        zq_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-6, affine=True)\n",
    "        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, f: torch.FloatTensor, zq: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        f_size = f.shape[-2:]\n",
    "        zq = F.interpolate(zq, size=f_size, mode=\"nearest\")\n",
    "        norm_f = self.norm_layer(f)\n",
    "        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)\n",
    "        return new_f\n",
    "    \n",
    "    \n",
    "    \n",
    "class ConvAttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels, t_emb_dim=512, dropout=0, nhead=8):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.dropout = dropout\n",
    "        self.nhead = in_channels//nhead\n",
    "        \n",
    "        self.norm = nn.GroupNorm(32, in_channels)\n",
    "        \n",
    "        #self.norm = SpatialNorm(in_channels, t_emb_dim)\n",
    "\n",
    "        self.q = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.k = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.v = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.proj_out = torch.nn.Linear(in_channels,\n",
    "                                        in_channels)\n",
    "        self.q.apply(init_cnn)\n",
    "        self.k.apply(init_cnn)\n",
    "        self.v.apply(init_cnn)\n",
    "        self.proj_out.apply(init_cnn)\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_).view(b, c, h*w).transpose(1,2)\n",
    "        \n",
    "        #h_ = self.norm(h_, t_emb)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        q = q.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "        k = k.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "        v = k.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "\n",
    "        # compute attention\n",
    "\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            h_ = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "\n",
    "        h_ = h_.transpose(1, 2).view(b, h*w, c)\n",
    "        h_ = self.proj_out(h_).transpose(1,2)\n",
    "\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        print(f\"{h_.shape}\")\n",
    "        #h_ = self.norm(h_, t_emb)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b, c, h, w = q.shape\n",
    "        q = q.view(b, c, h*w).transpose(1,2)\n",
    "        k = k.view(b, c, h*w).transpose(1,2)\n",
    "        v = v.view(b, c, h*w).transpose(1,2)\n",
    "        '''\n",
    "        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
    "        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = torch.bmm(v, w_)\n",
    "        '''\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            h_ = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "\n",
    "        h_ = h_.transpose(1, 2)\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_hiddens=512, med_hiddens=512, out_hiddens=512, layers=1,\n",
    "                 init=init_relu, in_act=nn.SiLU(), out_act=nn.Identity(),\n",
    "                 ln_eps=1e-3, last_init=init_xavier, bias=True):\n",
    "        super().__init__()\n",
    "        # Special MLP with custom options for non last layer and last layer Linears.\n",
    "\n",
    "        modules=[]\n",
    "        self.init=init\n",
    "        self.last_init=last_init\n",
    "        \n",
    "        hiddens=in_hiddens\n",
    "        _out_hiddens = med_hiddens\n",
    "        act = in_act\n",
    "        for l in range(layers):\n",
    "            last_layer = l==(layers-1)\n",
    "            if last_layer:\n",
    "                _out_hiddens = out_hiddens\n",
    "                act = out_act\n",
    "            modules.append(nn.Linear(hiddens, _out_hiddens, bias=bias))\n",
    "            \n",
    "            modules.append(act)\n",
    "            hiddens=med_hiddens\n",
    "        self.mlp=nn.Sequential(*modules)\n",
    "        #print(self.mlp)\n",
    "\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def turn_off_grads(self):\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                layer.weight.requires_grad=False\n",
    "            if hasattr(layer, 'bias'):\n",
    "                layer.bias.requires_grad=False\n",
    "    def init_weights(self):\n",
    "        self.mlp.apply(self.init)\n",
    "        self.mlp[-2].apply(self.last_init)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.mlp(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class ViT(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, patches=(16,16), img_size=(96,72), first_channel=3,\n",
    "                 dropout=0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = np.prod(patches)\n",
    "        self.N = int(np.prod(img_size) / self.patches)\n",
    "\n",
    "        self.in_proj = MLP(first_channel * self.patches, out_hiddens=d_model, last_init=init_gpt)\n",
    "\n",
    "        # Classe \"token\" de pooling\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.transformer = Transformer_NoDATA(\n",
    "            d_model, num_blks, nhead, seq_len=self.N + 1,\n",
    "            dropout=dropout, bias=bias, report_params_count=False,\n",
    "            ffn_mult=ffn_mult, stochastic_depth=stochastic_depth\n",
    "        )\n",
    "\n",
    "        # Inicializar pesos do CLS token\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'ViT')\n",
    "\n",
    "    def patchify(self, X):\n",
    "        # Dividir a imagem em patches e reformatar\n",
    "        X = X.view(-1, self.patches * self.first_channel, self.N).transpose(-2, -1)\n",
    "        return X\n",
    "\n",
    "    def proj(self, X):\n",
    "        X = self.patchify(X)\n",
    "        return self.in_proj(X)\n",
    "    \n",
    "    def transformers(self, X):\n",
    "        \n",
    "        X = self.transformer(X, is_causal=False).view(-1, self.stacked_frames*self.N, self.d_model)\n",
    "        X = self.temporal_aggr(X, is_causal=False)\n",
    "        \n",
    "        return X[:,-self.N:]\n",
    "\n",
    "    def masked(self, X, mask):\n",
    "        \n",
    "        X = self.transformer.masked(X, mask, is_causal=False).view(-1, self.stacked_frames*mask.shape[1], self.d_model)\n",
    "        X = self.temporal_aggr(X, is_causal=False)\n",
    "        \n",
    "        return X[:,-mask.shape[1]:]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Criar patches e projetar\n",
    "        X = self.patchify(X)\n",
    "        X = self.in_proj(X)\n",
    "\n",
    "        # Adicionar o token de classe\n",
    "        batch_size = X.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        X = torch.cat((cls_tokens, X), dim=1)\n",
    "\n",
    "        # Passar pelo transformer\n",
    "        X = self.transformer(X, is_causal=False)\n",
    "\n",
    "        # Retornar apenas o token de classe\n",
    "        return X[:, 0]  # Forma: [batch_size, d_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViT_IWM(nsd_Module):\n",
    "    def __init__(self, encoder,\n",
    "                 d_predictor, num_blks_predictor, nhead_predictor,\n",
    "                 stacked_frames=4,\n",
    "                 mask_samples=4,\n",
    "                 masked_tokens=4,\n",
    "                 num_augmentations=3,\n",
    "                 first_channel=3,\n",
    "                 dropout = 0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_encoder = encoder.d_model\n",
    "        \n",
    "        \n",
    "        self.first_channel = encoder.first_channel*stacked_frames\n",
    "        self.img_size = encoder.img_size\n",
    "        self.patches = encoder.patches\n",
    "        self.N = encoder.N\n",
    "        self.masked_tokens=self.N//masked_tokens\n",
    "\n",
    "        # Mask\n",
    "        self.mask = MLP(1, out_hiddens=d_predictor, last_init=init_xavier)\n",
    "        self.mask_pos_encoding = nn.Embedding(self.N, d_predictor)\n",
    "        self.mask_mlp = MLP(d_predictor+num_augmentations, d_predictor, d_predictor, layers=4, in_act=nn.ReLU(), out_act=nn.ReLU(),\n",
    "                            init=init_relu, last_init=init_gpt)\n",
    "        self.mask_pos_encoding.apply(init_gpt)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Predictor\n",
    "        self.predictor_proj = MLP(self.d_encoder, out_hiddens=d_predictor, last_init=init_gpt) \\\n",
    "                              if d_predictor!=self.d_encoder else nn.Identity()\n",
    "\n",
    "        self.predictor = Transformer_NoDATA(d_predictor, num_blks_predictor, nhead_predictor, seq_len=self.N+1,\n",
    "                 dropout = dropout, bias=bias, report_params_count=False,\n",
    "                 ffn_mult=ffn_mult, scale_init=num_blks_predictor, stochastic_depth=stochastic_depth)\n",
    "\n",
    "\n",
    "        self.predictor_out_proj = MLP(d_predictor, out_hiddens=self.d_encoder, last_init=init_gpt) \\\n",
    "                              if d_predictor!=self.d_encoder else nn.Identity()\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'IWM')\n",
    "\n",
    "    def hard_reset(self, new_network, alpha):\n",
    "        network_ema(self.encoder, new_network.encoder, alpha)\n",
    "\n",
    "        network_ema(self.predictor_proj, new_network.predictor_proj, alpha)\n",
    "        network_ema(self.predictor, new_network.predictor, alpha)\n",
    "\n",
    "        network_ema(self.mask, new_network.mask, alpha)\n",
    "        network_ema(self.mask_pos_encoding, new_network.mask_pos_encoding, alpha)\n",
    "        network_ema(self.mask_mlp, new_network.mask_mlp, alpha)\n",
    "\n",
    "    def get_random_mask(self, X, augmentations):\n",
    "        B, T, D = X.shape\n",
    "        B = B//self.stacked_frames\n",
    "        m_rand = self.mask_samples*random.randint(0,int(self.masked_tokens*2//self.mask_samples)-1)\n",
    "        \n",
    "        \n",
    "        # Get non-overlapping mask\n",
    "        mask_pos = torch.arange(T, device='cuda')[None,:].repeat_interleave(B,0).float()\n",
    "        mask_pos = torch.multinomial(mask_pos, num_samples=self.masked_tokens+m_rand, replacement=False)\n",
    "        \n",
    "        mask_pos_repeat = mask_pos.repeat_interleave(self.stacked_frames,0)\n",
    "\n",
    "        # Get the mask complement\n",
    "        full_range = torch.arange(T,device='cuda')[None,:].repeat_interleave(B,0)\n",
    "\n",
    "        complement = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "        complement.scatter_(1, mask_pos, 1)\n",
    "\n",
    "        complement = full_range[~complement].view(mask_pos.shape[0], -1)\n",
    "        \n",
    "\n",
    "        # Mask mlp for geometric + augmentation informations\n",
    "        mask = self.mask(torch.ones(B*self.stacked_frames,self.masked_tokens+m_rand,1, device='cuda'))\n",
    "\n",
    "        mask = mask + self.mask_pos_encoding(mask_pos_repeat)\n",
    "\n",
    "        augmentations = augmentations.repeat_interleave(self.stacked_frames,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "\n",
    "        mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "\n",
    "        # Expand to allow gather\n",
    "        mask_pos = mask_pos[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "        complement = complement[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "\n",
    "        return X, mask_pos, complement, mask\n",
    "        \n",
    "    def patchify(self, X):\n",
    "        X = X.view(-1, self.patches*self.first_channel, self.N).transpose(-2,-1)\n",
    "        return X\n",
    "    def get_block_mask(self, batch_size):\n",
    "        \n",
    "        all_wins = torch.zeros(self.first_channel,*self.img_size).long()\n",
    "        \n",
    "        b_mask, b_complement = [], []\n",
    "        min_c_len = 999 # for trunked collate\n",
    "        #min_m=999\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            wins, complements = [], []\n",
    "            for m in range(self.mask_samples):\n",
    "                w,h = self.img_size\n",
    "\n",
    "\n",
    "                min_ar, max_ar = (0.75, 1.5)\n",
    "                aspect_ratio = min_ar + random.random() * (max_ar - min_ar)\n",
    "\n",
    "                h_sample_size = int( (h*(torch.tensor(random.random())*0.05+0.15)) * aspect_ratio)\n",
    "\n",
    "                w_wins, h_wins = torch.randint(0,h-h_sample_size,(2,)).split(1,0)\n",
    "                win=all_wins.clone()\n",
    "\n",
    "\n",
    "                for w_win, h_win in zip(w_wins, h_wins):\n",
    "                    win[...,w_win:w_win+h_sample_size, h_win:h_win+h_sample_size]=1\n",
    "\n",
    "                \n",
    "                win = self.patchify(win.float()).mean(-1)\n",
    "                \n",
    "                values, idx = win.sort(descending=True)\n",
    "\n",
    "                idx = idx[:,:self.N//4]\n",
    "                \n",
    "                #min_m = min(min_m, len(values[0].nonzero()))\n",
    "                wins.append(idx)\n",
    "\n",
    "\n",
    "            wins = torch.stack(wins).squeeze()\n",
    "\n",
    "\n",
    "            full_range = torch.arange(win.shape[1])\n",
    "\n",
    "            complement = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "            complement.scatter_(0, wins.view(-1).unique(), 1)\n",
    "\n",
    "            complement = full_range[~complement]\n",
    "            min_c_len = min(min_c_len, len(complement))\n",
    "            \n",
    "            \n",
    "            b_mask.append(wins)\n",
    "            b_complement.append(complement)\n",
    "            \n",
    "            \n",
    "        for i in range(len(b_complement)):\n",
    "            b_complement[i] = b_complement[i][:min_c_len]\n",
    "        \n",
    "        b_mask = torch.stack(b_mask).cuda()\n",
    "        b_complement = torch.stack(b_complement).cuda()\n",
    "        #print(min_m)\n",
    "        \n",
    "        return b_mask, b_complement\n",
    "    \n",
    "    def get_mask(self, X, augmentations):\n",
    "        B = X.shape[0]//self.stacked_frames\n",
    "\n",
    "        \n",
    "        mask_pos, complement = self.get_block_mask(B)\n",
    "        mask_pos = mask_pos.view(B*self.mask_samples,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mask = self.mask(torch.ones(B*self.mask_samples,1,1, device='cuda'))\n",
    "        \n",
    "        mask = mask + self.mask_pos_encoding(mask_pos)\n",
    "        #augmentations = augmentations.repeat_interleave(self.stacked_frames*self.mask_samples,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "        #mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "\n",
    "\n",
    "        mask_pos = mask_pos[...,None].expand(-1,-1,self.d_encoder)\n",
    "        complement = complement[...,None].expand(-1,-1,self.d_encoder).repeat_interleave(self.stacked_frames,0)\n",
    "        \n",
    "        return mask_pos, mask, complement\n",
    "    \n",
    "    def encode(self, X):\n",
    "        return self.encoder(X)\n",
    "\n",
    "\n",
    "    def forward(self, X, y, augmentations):\n",
    "        X = self.encoder.proj(X)\n",
    "        \n",
    "        mask_pos, mask, complement = self.get_mask(X, augmentations)\n",
    "        \n",
    "        X = self.encoder.masked(X, complement)\n",
    "        X = self.predictor_proj(X)\n",
    "\n",
    "        X = torch.cat((X.repeat_interleave(4,0),mask),1)\n",
    "        \n",
    "        X = self.predictor.no_pos(X)[:,-mask.shape[1]:]\n",
    "        X = self.predictor_out_proj(X)\n",
    "        \n",
    "        return X, y.repeat_interleave(4,0).gather(1,mask_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_checkpoint_vit(modelo, classifier, otimizador, epoca, caminho):\n",
    "    checkpoint = {\n",
    "        'modelo': modelo.state_dict(),\n",
    "        'classifier': classifier.state_dict(),\n",
    "        'otimizador': otimizador.state_dict(),\n",
    "        'epoca': epoca\n",
    "    }\n",
    "    torch.save(checkpoint, caminho)\n",
    "    print(f\"Checkpoint salvo: {caminho}\")\n",
    "\n",
    "\n",
    "# Funo para carregar checkpoints\n",
    "def carregar_checkpoint_vit(modelo, classifier, otimizador, caminho):\n",
    "    if os.path.exists(caminho):\n",
    "        checkpoint = torch.load(caminho)\n",
    "        modelo.load_state_dict(checkpoint['modelo'])\n",
    "        classifier.load_state_dict(checkpoint['classifier'])\n",
    "        otimizador.load_state_dict(checkpoint['otimizador'])\n",
    "        epoca_inicial = checkpoint['epoca']\n",
    "        print(f\"Checkpoint carregado: {caminho} (poca {epoca_inicial})\")\n",
    "        return epoca_inicial\n",
    "    else:\n",
    "        print(f\"Nenhum checkpoint encontrado em: {caminho}. Comeando do zero.\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cuda:0\n",
      "ViT Parameters: 38.17M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15480\\1993890487.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(caminho)\n",
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15480\\2411256981.py:93: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Inicializar GradScaler para Mixed Precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint carregado: ../checkpoints/ultimo_checkpoint_nosavedata01.pth (poca 193)\n",
      "\n",
      "poca 194/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando:   0%|          | 0/449 [00:00<?, ?it/s]C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15480\\2411256981.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Mixed Precision\n",
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15480\\2782951431.py:168: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
      "Treinando: 100%|| 449/449 [07:16<00:00,  1.03it/s]\n",
      "Validando:   0%|          | 0/57 [00:00<?, ?it/s]C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15480\\2411256981.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9456, Acurcia: 0.6469\n",
      "Validao - Erro: 1.8062, Acurcia: 0.4558\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 27.62%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 58.77%\n",
      "Classe 4: 41.35%\n",
      "Classe 5: 43.64%\n",
      "Classe 6: 64.10%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 195/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:14<00:00,  1.03it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9462, Acurcia: 0.6466\n",
      "Validao - Erro: 1.8685, Acurcia: 0.4575\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 30.84%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 26.21%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 36.90%\n",
      "Classe 5: 41.65%\n",
      "Classe 6: 64.34%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 196/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9559, Acurcia: 0.6447\n",
      "Validao - Erro: 1.7853, Acurcia: 0.4631\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 28.48%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 39.92%\n",
      "Classe 3: 61.01%\n",
      "Classe 4: 39.21%\n",
      "Classe 5: 40.89%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 197/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.03it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9384, Acurcia: 0.6456\n",
      "Validao - Erro: 1.7846, Acurcia: 0.4575\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 40.90%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 28.63%\n",
      "Classe 3: 58.99%\n",
      "Classe 4: 37.89%\n",
      "Classe 5: 38.13%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 198/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.03it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9380, Acurcia: 0.6510\n",
      "Validao - Erro: 1.8308, Acurcia: 0.4464\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.47%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.87%\n",
      "Classe 3: 57.77%\n",
      "Classe 4: 41.02%\n",
      "Classe 5: 31.55%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 199/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9391, Acurcia: 0.6483\n",
      "Validao - Erro: 1.7830, Acurcia: 0.4692\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 29.12%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 69.39%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 33.84%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 200/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9281, Acurcia: 0.6515\n",
      "Validao - Erro: 1.8076, Acurcia: 0.4703\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 31.48%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.27%\n",
      "Classe 3: 65.92%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 35.22%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 201/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9250, Acurcia: 0.6543\n",
      "Validao - Erro: 1.8163, Acurcia: 0.4595\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 30.62%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 29.03%\n",
      "Classe 3: 65.59%\n",
      "Classe 4: 37.89%\n",
      "Classe 5: 37.67%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 202/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9215, Acurcia: 0.6559\n",
      "Validao - Erro: 1.9164, Acurcia: 0.4648\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 27.19%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 39.31%\n",
      "Classe 3: 61.79%\n",
      "Classe 4: 42.67%\n",
      "Classe 5: 34.30%\n",
      "Classe 6: 69.64%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 203/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9211, Acurcia: 0.6565\n",
      "Validao - Erro: 1.8430, Acurcia: 0.4700\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.48%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 35.28%\n",
      "Classe 3: 63.91%\n",
      "Classe 4: 40.53%\n",
      "Classe 5: 34.92%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 204/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9092, Acurcia: 0.6611\n",
      "Validao - Erro: 1.8578, Acurcia: 0.4536\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 32.26%\n",
      "Classe 3: 60.22%\n",
      "Classe 4: 33.77%\n",
      "Classe 5: 38.90%\n",
      "Classe 6: 68.67%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 205/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9144, Acurcia: 0.6556\n",
      "Validao - Erro: 1.8412, Acurcia: 0.4717\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.33%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 30.44%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 42.83%\n",
      "Classe 5: 37.83%\n",
      "Classe 6: 69.40%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 206/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9142, Acurcia: 0.6590\n",
      "Validao - Erro: 1.9136, Acurcia: 0.4597\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.83%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 67.37%\n",
      "Classe 4: 40.86%\n",
      "Classe 5: 31.70%\n",
      "Classe 6: 62.17%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 207/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9091, Acurcia: 0.6602\n",
      "Validao - Erro: 1.9018, Acurcia: 0.4642\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 29.44%\n",
      "Classe 3: 67.49%\n",
      "Classe 4: 43.00%\n",
      "Classe 5: 33.08%\n",
      "Classe 6: 61.20%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 208/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9016, Acurcia: 0.6625\n",
      "Validao - Erro: 1.8905, Acurcia: 0.4617\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.33%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 34.07%\n",
      "Classe 3: 70.73%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 31.70%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 209/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:33<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8940, Acurcia: 0.6634\n",
      "Validao - Erro: 1.8535, Acurcia: 0.4648\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 26.55%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 39.72%\n",
      "Classe 3: 66.59%\n",
      "Classe 4: 40.36%\n",
      "Classe 5: 36.91%\n",
      "Classe 6: 60.00%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 210/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8871, Acurcia: 0.6684\n",
      "Validao - Erro: 1.9428, Acurcia: 0.4592\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 30.41%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 28.83%\n",
      "Classe 3: 69.05%\n",
      "Classe 4: 37.07%\n",
      "Classe 5: 30.63%\n",
      "Classe 6: 73.25%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 211/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8967, Acurcia: 0.6664\n",
      "Validao - Erro: 1.8436, Acurcia: 0.4544\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.47%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 65.70%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 30.02%\n",
      "Classe 6: 62.89%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 212/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8833, Acurcia: 0.6700\n",
      "Validao - Erro: 1.8945, Acurcia: 0.4611\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.76%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 29.03%\n",
      "Classe 3: 62.79%\n",
      "Classe 4: 42.17%\n",
      "Classe 5: 34.00%\n",
      "Classe 6: 71.81%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 213/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8785, Acurcia: 0.6756\n",
      "Validao - Erro: 1.9227, Acurcia: 0.4636\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 30.62%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 32.06%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 43.66%\n",
      "Classe 5: 33.23%\n",
      "Classe 6: 66.99%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 214/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8713, Acurcia: 0.6741\n",
      "Validao - Erro: 1.8869, Acurcia: 0.4667\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.05%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 27.42%\n",
      "Classe 3: 65.03%\n",
      "Classe 4: 41.85%\n",
      "Classe 5: 35.07%\n",
      "Classe 6: 71.08%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 215/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8664, Acurcia: 0.6770\n",
      "Validao - Erro: 1.9225, Acurcia: 0.4645\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 24.40%\n",
      "Classe 3: 59.55%\n",
      "Classe 4: 51.40%\n",
      "Classe 5: 39.20%\n",
      "Classe 6: 64.10%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 216/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8723, Acurcia: 0.6764\n",
      "Validao - Erro: 1.8810, Acurcia: 0.4631\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 40.26%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.47%\n",
      "Classe 3: 61.12%\n",
      "Classe 4: 37.56%\n",
      "Classe 5: 34.30%\n",
      "Classe 6: 68.92%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 217/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8586, Acurcia: 0.6822\n",
      "Validao - Erro: 1.9174, Acurcia: 0.4614\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.76%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 32.06%\n",
      "Classe 3: 64.36%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 39.36%\n",
      "Classe 6: 64.58%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 218/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8566, Acurcia: 0.6805\n",
      "Validao - Erro: 2.0087, Acurcia: 0.4550\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 29.55%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 35.69%\n",
      "Classe 3: 59.22%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 38.74%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 219/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8562, Acurcia: 0.6807\n",
      "Validao - Erro: 2.0527, Acurcia: 0.4583\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 35.28%\n",
      "Classe 3: 64.47%\n",
      "Classe 4: 40.53%\n",
      "Classe 5: 29.56%\n",
      "Classe 6: 64.82%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 220/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:11<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8545, Acurcia: 0.6788\n",
      "Validao - Erro: 1.8623, Acurcia: 0.4723\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.40%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.06%\n",
      "Classe 3: 65.70%\n",
      "Classe 4: 43.49%\n",
      "Classe 5: 35.99%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 221/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8349, Acurcia: 0.6883\n",
      "Validao - Erro: 2.0311, Acurcia: 0.4606\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 37.30%\n",
      "Classe 3: 56.42%\n",
      "Classe 4: 44.32%\n",
      "Classe 5: 37.98%\n",
      "Classe 6: 62.17%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 222/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:13<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8318, Acurcia: 0.6920\n",
      "Validao - Erro: 1.9444, Acurcia: 0.4664\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 31.25%\n",
      "Classe 3: 64.47%\n",
      "Classe 4: 35.58%\n",
      "Classe 5: 40.12%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 223/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:12<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8444, Acurcia: 0.6852\n",
      "Validao - Erro: 1.9770, Acurcia: 0.4656\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 30.84%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 31.05%\n",
      "Classe 3: 69.05%\n",
      "Classe 4: 41.35%\n",
      "Classe 5: 27.87%\n",
      "Classe 6: 72.05%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 224/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8406, Acurcia: 0.6872\n",
      "Validao - Erro: 1.9151, Acurcia: 0.4717\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.55%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 35.08%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 40.53%\n",
      "Classe 5: 37.37%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 225/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8337, Acurcia: 0.6919\n",
      "Validao - Erro: 1.9476, Acurcia: 0.4622\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 31.05%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 27.62%\n",
      "Classe 3: 68.16%\n",
      "Classe 4: 36.08%\n",
      "Classe 5: 36.91%\n",
      "Classe 6: 69.64%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 226/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:11<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8274, Acurcia: 0.6938\n",
      "Validao - Erro: 1.9677, Acurcia: 0.4611\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.83%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 38.10%\n",
      "Classe 3: 65.36%\n",
      "Classe 4: 43.00%\n",
      "Classe 5: 30.47%\n",
      "Classe 6: 58.80%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 227/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:11<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8138, Acurcia: 0.6970\n",
      "Validao - Erro: 2.1277, Acurcia: 0.4628\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.55%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.27%\n",
      "Classe 3: 63.24%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 67.23%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 228/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8234, Acurcia: 0.6959\n",
      "Validao - Erro: 2.0007, Acurcia: 0.4689\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.12%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 66.70%\n",
      "Classe 4: 39.04%\n",
      "Classe 5: 32.47%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 229/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8182, Acurcia: 0.6983\n",
      "Validao - Erro: 2.0923, Acurcia: 0.4617\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 29.84%\n",
      "Classe 3: 66.70%\n",
      "Classe 4: 39.70%\n",
      "Classe 5: 33.38%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 230/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8124, Acurcia: 0.6984\n",
      "Validao - Erro: 2.0025, Acurcia: 0.4673\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.33%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 36.49%\n",
      "Classe 3: 65.47%\n",
      "Classe 4: 38.55%\n",
      "Classe 5: 32.62%\n",
      "Classe 6: 69.64%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 231/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8086, Acurcia: 0.7011\n",
      "Validao - Erro: 2.0266, Acurcia: 0.4650\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 39.31%\n",
      "Classe 3: 65.25%\n",
      "Classe 4: 38.06%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 60.24%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 232/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8117, Acurcia: 0.6979\n",
      "Validao - Erro: 2.0131, Acurcia: 0.4759\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.97%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 32.86%\n",
      "Classe 3: 68.83%\n",
      "Classe 4: 40.36%\n",
      "Classe 5: 34.92%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 233/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7994, Acurcia: 0.7038\n",
      "Validao - Erro: 2.0655, Acurcia: 0.4622\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 27.42%\n",
      "Classe 3: 61.68%\n",
      "Classe 4: 39.21%\n",
      "Classe 5: 39.51%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 234/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8057, Acurcia: 0.7023\n",
      "Validao - Erro: 2.1233, Acurcia: 0.4681\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.97%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 30.24%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 36.57%\n",
      "Classe 5: 41.81%\n",
      "Classe 6: 63.13%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 235/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7869, Acurcia: 0.7078\n",
      "Validao - Erro: 2.1222, Acurcia: 0.4664\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 29.76%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 41.94%\n",
      "Classe 3: 64.02%\n",
      "Classe 4: 40.86%\n",
      "Classe 5: 32.92%\n",
      "Classe 6: 64.82%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 236/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7953, Acurcia: 0.7054\n",
      "Validao - Erro: 1.9668, Acurcia: 0.4717\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.62%\n",
      "Classe 1: 51.79%\n",
      "Classe 2: 32.06%\n",
      "Classe 3: 63.91%\n",
      "Classe 4: 38.55%\n",
      "Classe 5: 37.67%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 237/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7839, Acurcia: 0.7123\n",
      "Validao - Erro: 2.0413, Acurcia: 0.4731\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.26%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 36.90%\n",
      "Classe 3: 64.92%\n",
      "Classe 4: 38.71%\n",
      "Classe 5: 36.75%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 238/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7833, Acurcia: 0.7097\n",
      "Validao - Erro: 2.0984, Acurcia: 0.4823\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.19%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 67.93%\n",
      "Classe 4: 42.34%\n",
      "Classe 5: 36.60%\n",
      "Classe 6: 68.92%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 239/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7821, Acurcia: 0.7117\n",
      "Validao - Erro: 2.1367, Acurcia: 0.4703\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 41.76%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 30.44%\n",
      "Classe 3: 65.25%\n",
      "Classe 4: 37.89%\n",
      "Classe 5: 35.83%\n",
      "Classe 6: 64.34%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 240/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7757, Acurcia: 0.7157\n",
      "Validao - Erro: 2.0896, Acurcia: 0.4706\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.62%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 31.45%\n",
      "Classe 3: 67.60%\n",
      "Classe 4: 42.34%\n",
      "Classe 5: 34.00%\n",
      "Classe 6: 61.45%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 241/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7675, Acurcia: 0.7159\n",
      "Validao - Erro: 2.0417, Acurcia: 0.4767\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.69%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 30.65%\n",
      "Classe 3: 68.94%\n",
      "Classe 4: 37.23%\n",
      "Classe 5: 36.29%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 242/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7851, Acurcia: 0.7096\n",
      "Validao - Erro: 2.1185, Acurcia: 0.4695\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 39.61%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 35.89%\n",
      "Classe 3: 66.15%\n",
      "Classe 4: 36.57%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 243/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7688, Acurcia: 0.7136\n",
      "Validao - Erro: 2.0283, Acurcia: 0.4656\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 37.50%\n",
      "Classe 3: 60.22%\n",
      "Classe 4: 41.35%\n",
      "Classe 5: 35.83%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 244/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7667, Acurcia: 0.7169\n",
      "Validao - Erro: 2.0773, Acurcia: 0.4728\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 36.09%\n",
      "Classe 3: 61.01%\n",
      "Classe 4: 43.66%\n",
      "Classe 5: 38.28%\n",
      "Classe 6: 63.37%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 245/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7491, Acurcia: 0.7259\n",
      "Validao - Erro: 2.2110, Acurcia: 0.4656\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.12%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 32.86%\n",
      "Classe 3: 68.04%\n",
      "Classe 4: 31.63%\n",
      "Classe 5: 32.92%\n",
      "Classe 6: 73.25%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 246/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7535, Acurcia: 0.7215\n",
      "Validao - Erro: 2.1311, Acurcia: 0.4801\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.33%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 68.16%\n",
      "Classe 4: 43.33%\n",
      "Classe 5: 35.53%\n",
      "Classe 6: 67.71%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 247/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:11<00:00,  1.04it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7541, Acurcia: 0.7217\n",
      "Validao - Erro: 2.1026, Acurcia: 0.4689\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 34.48%\n",
      "Classe 3: 63.91%\n",
      "Classe 4: 37.40%\n",
      "Classe 5: 37.37%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 248/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:09<00:00,  1.05it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7592, Acurcia: 0.7212\n",
      "Validao - Erro: 2.1234, Acurcia: 0.4714\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 30.65%\n",
      "Classe 3: 63.46%\n",
      "Classe 4: 43.00%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 249/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:30<00:00,  1.00s/it]\n",
      "Validando: 100%|| 57/57 [00:33<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7483, Acurcia: 0.7236\n",
      "Validao - Erro: 2.0804, Acurcia: 0.4675\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 31.91%\n",
      "Classe 1: 53.57%\n",
      "Classe 2: 38.71%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 34.93%\n",
      "Classe 5: 37.83%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 250/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:51<00:00,  1.05s/it]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7508, Acurcia: 0.7228\n",
      "Validao - Erro: 2.1048, Acurcia: 0.4692\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.12%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 34.27%\n",
      "Classe 3: 63.69%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 39.66%\n",
      "Classe 6: 63.13%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 251/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:27<00:00,  1.00it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7375, Acurcia: 0.7278\n",
      "Validao - Erro: 2.1337, Acurcia: 0.4723\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 32.86%\n",
      "Classe 3: 64.58%\n",
      "Classe 4: 43.82%\n",
      "Classe 5: 34.00%\n",
      "Classe 6: 66.99%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 252/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:32<00:00,  1.01s/it]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7297, Acurcia: 0.7288\n",
      "Validao - Erro: 2.2223, Acurcia: 0.4728\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 33.67%\n",
      "Classe 3: 64.58%\n",
      "Classe 4: 37.07%\n",
      "Classe 5: 42.11%\n",
      "Classe 6: 63.37%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 253/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:33<00:00,  1.01s/it]\n",
      "Validando: 100%|| 57/57 [00:33<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7408, Acurcia: 0.7279\n",
      "Validao - Erro: 2.1723, Acurcia: 0.4592\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.48%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 35.89%\n",
      "Classe 3: 59.55%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 32.62%\n",
      "Classe 6: 68.67%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 254/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:30<00:00,  1.00s/it]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7322, Acurcia: 0.7284\n",
      "Validao - Erro: 2.1856, Acurcia: 0.4620\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.33%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 35.69%\n",
      "Classe 3: 63.24%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 32.47%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 255/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:05<00:00,  1.06it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7261, Acurcia: 0.7304\n",
      "Validao - Erro: 2.2446, Acurcia: 0.4731\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.62%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 31.65%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 45.47%\n",
      "Classe 5: 33.69%\n",
      "Classe 6: 65.78%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 256/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:55<00:00,  1.08it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7245, Acurcia: 0.7315\n",
      "Validao - Erro: 2.1258, Acurcia: 0.4700\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.12%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 37.30%\n",
      "Classe 3: 61.34%\n",
      "Classe 4: 38.55%\n",
      "Classe 5: 41.65%\n",
      "Classe 6: 62.41%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 257/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:21<00:00,  1.02it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7112, Acurcia: 0.7367\n",
      "Validao - Erro: 2.1758, Acurcia: 0.4700\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 31.69%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 40.93%\n",
      "Classe 3: 59.11%\n",
      "Classe 4: 41.02%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 67.71%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 258/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:18<00:00,  1.02it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7106, Acurcia: 0.7375\n",
      "Validao - Erro: 2.2066, Acurcia: 0.4544\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.76%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 34.27%\n",
      "Classe 3: 59.66%\n",
      "Classe 4: 40.03%\n",
      "Classe 5: 33.08%\n",
      "Classe 6: 70.36%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 259/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:59<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7125, Acurcia: 0.7382\n",
      "Validao - Erro: 2.2698, Acurcia: 0.4642\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 29.34%\n",
      "Classe 1: 51.79%\n",
      "Classe 2: 36.49%\n",
      "Classe 3: 65.81%\n",
      "Classe 4: 36.08%\n",
      "Classe 5: 34.92%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 260/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:58<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7042, Acurcia: 0.7402\n",
      "Validao - Erro: 2.1883, Acurcia: 0.4709\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.69%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 37.30%\n",
      "Classe 3: 61.90%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 60.48%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 261/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:17<00:00,  1.03it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6986, Acurcia: 0.7421\n",
      "Validao - Erro: 2.2681, Acurcia: 0.4661\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 39.19%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 37.70%\n",
      "Classe 3: 55.53%\n",
      "Classe 4: 37.07%\n",
      "Classe 5: 40.12%\n",
      "Classe 6: 70.12%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 262/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:20<00:00,  1.02it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7005, Acurcia: 0.7434\n",
      "Validao - Erro: 2.2833, Acurcia: 0.4653\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 29.34%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 33.67%\n",
      "Classe 3: 63.35%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 40.12%\n",
      "Classe 6: 64.10%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 263/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:23<00:00,  1.01it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6919, Acurcia: 0.7450\n",
      "Validao - Erro: 2.2305, Acurcia: 0.4773\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.62%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 34.07%\n",
      "Classe 3: 67.71%\n",
      "Classe 4: 43.00%\n",
      "Classe 5: 30.63%\n",
      "Classe 6: 67.23%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 264/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:57<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6913, Acurcia: 0.7476\n",
      "Validao - Erro: 2.3125, Acurcia: 0.4684\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.26%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 37.10%\n",
      "Classe 3: 63.80%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 37.06%\n",
      "Classe 6: 63.37%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 265/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:58<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6802, Acurcia: 0.7482\n",
      "Validao - Erro: 2.3245, Acurcia: 0.4606\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 32.26%\n",
      "Classe 3: 61.90%\n",
      "Classe 4: 37.73%\n",
      "Classe 5: 35.83%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 266/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:50<00:00,  1.09it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6807, Acurcia: 0.7489\n",
      "Validao - Erro: 2.3535, Acurcia: 0.4636\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.83%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 30.65%\n",
      "Classe 3: 62.91%\n",
      "Classe 4: 37.89%\n",
      "Classe 5: 43.34%\n",
      "Classe 6: 60.48%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 267/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:48<00:00,  1.10it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6850, Acurcia: 0.7478\n",
      "Validao - Erro: 2.2386, Acurcia: 0.4831\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 32.76%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 37.10%\n",
      "Classe 3: 67.49%\n",
      "Classe 4: 43.49%\n",
      "Classe 5: 36.14%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "poca 268/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:46<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6716, Acurcia: 0.7528\n",
      "Validao - Erro: 2.2088, Acurcia: 0.4650\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.04%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 32.86%\n",
      "Classe 3: 57.09%\n",
      "Classe 4: 42.67%\n",
      "Classe 5: 38.90%\n",
      "Classe 6: 68.92%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 269/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:42<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6761, Acurcia: 0.7530\n",
      "Validao - Erro: 2.2070, Acurcia: 0.4628\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.90%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 33.27%\n",
      "Classe 3: 59.44%\n",
      "Classe 4: 39.21%\n",
      "Classe 5: 41.04%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 270/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:45<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6724, Acurcia: 0.7504\n",
      "Validao - Erro: 2.2334, Acurcia: 0.4661\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.83%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 33.06%\n",
      "Classe 3: 63.02%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 33.69%\n",
      "Classe 6: 69.88%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 271/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:46<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6747, Acurcia: 0.7501\n",
      "Validao - Erro: 2.2801, Acurcia: 0.4634\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.40%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 35.69%\n",
      "Classe 3: 62.79%\n",
      "Classe 4: 39.54%\n",
      "Classe 5: 37.83%\n",
      "Classe 6: 58.55%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 272/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:42<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6713, Acurcia: 0.7549\n",
      "Validao - Erro: 2.3352, Acurcia: 0.4792\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 37.30%\n",
      "Classe 3: 65.25%\n",
      "Classe 4: 36.57%\n",
      "Classe 5: 40.12%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 273/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:02<00:00,  1.06it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6632, Acurcia: 0.7571\n",
      "Validao - Erro: 2.4422, Acurcia: 0.4734\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.62%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 42.94%\n",
      "Classe 3: 62.68%\n",
      "Classe 4: 41.02%\n",
      "Classe 5: 33.69%\n",
      "Classe 6: 65.54%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 274/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6608, Acurcia: 0.7586\n",
      "Validao - Erro: 2.3375, Acurcia: 0.4673\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 43.47%\n",
      "Classe 1: 48.21%\n",
      "Classe 2: 42.74%\n",
      "Classe 3: 60.45%\n",
      "Classe 4: 34.10%\n",
      "Classe 5: 32.92%\n",
      "Classe 6: 65.54%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 275/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:42<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6573, Acurcia: 0.7606\n",
      "Validao - Erro: 2.4667, Acurcia: 0.4739\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.12%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 36.69%\n",
      "Classe 3: 67.93%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 28.33%\n",
      "Classe 6: 65.78%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 276/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:41<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6535, Acurcia: 0.7612\n",
      "Validao - Erro: 2.3026, Acurcia: 0.4776\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 41.76%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 38.51%\n",
      "Classe 3: 61.68%\n",
      "Classe 4: 42.01%\n",
      "Classe 5: 35.99%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 277/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:42<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6602, Acurcia: 0.7576\n",
      "Validao - Erro: 2.3197, Acurcia: 0.4767\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.83%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 34.88%\n",
      "Classe 3: 70.06%\n",
      "Classe 4: 34.10%\n",
      "Classe 5: 36.75%\n",
      "Classe 6: 64.58%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 278/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:38<00:00,  1.13it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6422, Acurcia: 0.7669\n",
      "Validao - Erro: 2.4204, Acurcia: 0.4622\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.90%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 42.94%\n",
      "Classe 3: 62.12%\n",
      "Classe 4: 33.11%\n",
      "Classe 5: 34.30%\n",
      "Classe 6: 66.99%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 279/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:38<00:00,  1.13it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6394, Acurcia: 0.7632\n",
      "Validao - Erro: 2.3324, Acurcia: 0.4809\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.19%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 34.48%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 42.34%\n",
      "Classe 5: 44.26%\n",
      "Classe 6: 61.20%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 280/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:36<00:00,  1.13it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6433, Acurcia: 0.7648\n",
      "Validao - Erro: 2.3643, Acurcia: 0.4656\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 34.88%\n",
      "Classe 3: 62.91%\n",
      "Classe 4: 40.20%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 281/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:39<00:00,  1.13it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6432, Acurcia: 0.7663\n",
      "Validao - Erro: 2.3293, Acurcia: 0.4748\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 63.69%\n",
      "Classe 4: 40.03%\n",
      "Classe 5: 39.51%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 282/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:49<00:00,  1.10it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6409, Acurcia: 0.7667\n",
      "Validao - Erro: 2.3736, Acurcia: 0.4737\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.04%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 36.49%\n",
      "Classe 3: 63.91%\n",
      "Classe 4: 40.03%\n",
      "Classe 5: 37.98%\n",
      "Classe 6: 62.17%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 283/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:38<00:00,  1.13it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6497, Acurcia: 0.7642\n",
      "Validao - Erro: 2.3507, Acurcia: 0.4765\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 34.48%\n",
      "Classe 1: 51.79%\n",
      "Classe 2: 38.91%\n",
      "Classe 3: 66.59%\n",
      "Classe 4: 37.40%\n",
      "Classe 5: 33.08%\n",
      "Classe 6: 69.40%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 284/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:45<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6271, Acurcia: 0.7688\n",
      "Validao - Erro: 2.3987, Acurcia: 0.4767\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.12%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 62.79%\n",
      "Classe 4: 45.63%\n",
      "Classe 5: 35.99%\n",
      "Classe 6: 68.92%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 285/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:40<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6256, Acurcia: 0.7700\n",
      "Validao - Erro: 2.3352, Acurcia: 0.4790\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.54%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 35.69%\n",
      "Classe 3: 64.02%\n",
      "Classe 4: 43.66%\n",
      "Classe 5: 34.92%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 286/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:45<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6274, Acurcia: 0.7705\n",
      "Validao - Erro: 2.4411, Acurcia: 0.4611\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.47%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 35.89%\n",
      "Classe 3: 57.65%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 39.36%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 287/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:39<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6214, Acurcia: 0.7738\n",
      "Validao - Erro: 2.3138, Acurcia: 0.4778\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.90%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 35.48%\n",
      "Classe 3: 64.92%\n",
      "Classe 4: 40.20%\n",
      "Classe 5: 38.90%\n",
      "Classe 6: 62.17%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 288/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:41<00:00,  1.12it/s]\n",
      "Validando: 100%|| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6110, Acurcia: 0.7766\n",
      "Validao - Erro: 2.3666, Acurcia: 0.4762\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 41.11%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 36.49%\n",
      "Classe 3: 64.02%\n",
      "Classe 4: 35.42%\n",
      "Classe 5: 40.28%\n",
      "Classe 6: 62.41%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 289/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:46<00:00,  1.10it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6199, Acurcia: 0.7742\n",
      "Validao - Erro: 2.3845, Acurcia: 0.4687\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.76%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 35.48%\n",
      "Classe 3: 64.58%\n",
      "Classe 4: 42.17%\n",
      "Classe 5: 32.01%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 290/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:48<00:00,  1.10it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6145, Acurcia: 0.7756\n",
      "Validao - Erro: 2.4284, Acurcia: 0.4734\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.12%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 40.12%\n",
      "Classe 3: 61.68%\n",
      "Classe 4: 39.54%\n",
      "Classe 5: 37.06%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 291/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:06<00:00,  1.05it/s]\n",
      "Validando: 100%|| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6100, Acurcia: 0.7772\n",
      "Validao - Erro: 2.4555, Acurcia: 0.4684\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 38.54%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 33.67%\n",
      "Classe 3: 63.69%\n",
      "Classe 4: 41.85%\n",
      "Classe 5: 34.61%\n",
      "Classe 6: 63.37%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 292/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:57<00:00,  1.08it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6090, Acurcia: 0.7771\n",
      "Validao - Erro: 2.5013, Acurcia: 0.4670\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 36.90%\n",
      "Classe 3: 60.56%\n",
      "Classe 4: 39.37%\n",
      "Classe 5: 40.43%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 293/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:03<00:00,  1.06it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6063, Acurcia: 0.7786\n",
      "Validao - Erro: 2.4926, Acurcia: 0.4726\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.76%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 39.11%\n",
      "Classe 3: 61.12%\n",
      "Classe 4: 43.16%\n",
      "Classe 5: 33.54%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 294/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:47<00:00,  1.10it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6056, Acurcia: 0.7792\n",
      "Validao - Erro: 2.3003, Acurcia: 0.4592\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.69%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 39.31%\n",
      "Classe 3: 60.89%\n",
      "Classe 4: 29.98%\n",
      "Classe 5: 40.28%\n",
      "Classe 6: 62.41%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 295/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:45<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5917, Acurcia: 0.7834\n",
      "Validao - Erro: 2.5286, Acurcia: 0.4717\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 36.19%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 42.54%\n",
      "Classe 3: 60.89%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 34.30%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 296/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:45<00:00,  1.11it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5948, Acurcia: 0.7832\n",
      "Validao - Erro: 2.4964, Acurcia: 0.4809\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.55%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 34.68%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 39.70%\n",
      "Classe 5: 39.82%\n",
      "Classe 6: 67.23%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 297/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:18<00:00,  1.03it/s]\n",
      "Validando: 100%|| 57/57 [00:32<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5967, Acurcia: 0.7836\n",
      "Validao - Erro: 2.4857, Acurcia: 0.4661\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 41.11%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 37.70%\n",
      "Classe 3: 59.55%\n",
      "Classe 4: 38.06%\n",
      "Classe 5: 37.98%\n",
      "Classe 6: 62.41%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 298/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:59<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5903, Acurcia: 0.7842\n",
      "Validao - Erro: 2.4369, Acurcia: 0.4745\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 37.47%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 35.48%\n",
      "Classe 3: 64.47%\n",
      "Classe 4: 46.46%\n",
      "Classe 5: 28.33%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 299/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [07:00<00:00,  1.07it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5886, Acurcia: 0.7819\n",
      "Validao - Erro: 2.4249, Acurcia: 0.4689\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 33.40%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 37.30%\n",
      "Classe 3: 61.56%\n",
      "Classe 4: 40.03%\n",
      "Classe 5: 38.59%\n",
      "Classe 6: 65.78%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n",
      "\n",
      "poca 300/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|| 449/449 [06:54<00:00,  1.08it/s]\n",
      "Validando: 100%|| 57/57 [00:30<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5937, Acurcia: 0.7840\n",
      "Validao - Erro: 2.5008, Acurcia: 0.4717\n",
      "Acurcia por classe na validao:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 44.64%\n",
      "Classe 2: 37.10%\n",
      "Classe 3: 59.66%\n",
      "Classe 4: 39.37%\n",
      "Classe 5: 40.12%\n",
      "Classe 6: 68.43%\n",
      "Checkpoint salvo: ../checkpoints/ultimo_checkpoint_nosavedata01.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configurao do dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# Configuraes gerais\n",
    "numero_de_epocas = 300\n",
    "bs = 64\n",
    "image_size = (96, 72)  # Atualizado para corresponder ao modelo customizado\n",
    "patches = (16, 16)  # Tamanho do patch do ViT\n",
    "num_classes = 7  # Atualize de acordo com seu dataset\n",
    "checkpoint_dir = '../checkpoints/'\n",
    "\n",
    "# Criar diretrio de checkpoints se no existir\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Transformaes para as imagens\n",
    "transformacoes_de_imagens = {\n",
    "    'treino': transforms.Compose([\n",
    "        transforms.Resize(size=image_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "\n",
    "    'validacao': transforms.Compose([\n",
    "        transforms.Resize(size=image_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Carregar datasets\n",
    "dataset = '../data/Fer-2013/'\n",
    "pasta_treino = os.path.join(dataset, 'treino')\n",
    "pasta_validacao = os.path.join(dataset, 'validacao')\n",
    "\n",
    "data = {\n",
    "    'treino': datasets.ImageFolder(root=pasta_treino, transform=transformacoes_de_imagens['treino']),\n",
    "    'validacao': datasets.ImageFolder(root=pasta_validacao, transform=transformacoes_de_imagens['validacao'])\n",
    "}\n",
    "\n",
    "# Criar DataLoaders\n",
    "data_loader_treino = DataLoader(data['treino'], batch_size=bs, shuffle=True, num_workers=4)\n",
    "data_loader_validacao = DataLoader(data['validacao'], batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "# Definir o modelo ViT customizado\n",
    " \n",
    "\n",
    "vit_model = ViT(\n",
    "    d_model=512,  # Dimenso do modelo\n",
    "    num_blks=12,  # Nmero de blocos do transformer\n",
    "    nhead=8,  # Nmero de cabeas de ateno\n",
    "    patches=patches,\n",
    "    img_size=image_size,\n",
    "    first_channel=3,  # Nmero de canais de entrada\n",
    "    dropout=0.1,\n",
    "    report_params_count=True\n",
    ")\n",
    "\n",
    "# Classificador para ajustar a sada do ViT\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "\n",
    "vit_model.to(device)\n",
    "classifier.to(device)\n",
    "\n",
    "# Definir a funo de erro e o otimizador\n",
    "funcao_erro = nn.CrossEntropyLoss()  # Negative Log Likelihood Loss\n",
    "otimizador = optim.AdamW(\n",
    "    list(vit_model.parameters()) + list(classifier.parameters()), \n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'ultimo_checkpoint_nosavedata01.pth')\n",
    "epoca_inicial = carregar_checkpoint_vit(vit_model, classifier, otimizador, checkpoint_path)\n",
    "\n",
    "# Atualizar a funo `treinar_e_validar` para salvar sempre no mesmo arquivo o ltimo checkpoint\n",
    "def treinar_e_validar(modelo, classifier, metrica_erro, otimizador_sgd, epocas=25, iniciar_epoca=0, melhor_acuracia=0.0):\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Inicializar GradScaler para Mixed Precision\n",
    "    historico = []\n",
    "\n",
    "    for epoca in range(iniciar_epoca, epocas):\n",
    "        inicio_epoca = time.time()\n",
    "        print(f\"\\npoca {epoca + 1}/{epocas}\")\n",
    "\n",
    "        # Modo de treinamento\n",
    "        modelo.train()\n",
    "        classifier.train()\n",
    "        erro_treino = 0.0\n",
    "        acuracia_treino = 0.0\n",
    "\n",
    "        for entradas, labels in tqdm(data_loader_treino, desc=\"Treinando\"):\n",
    "            entradas, labels = entradas.to(device), labels.to(device)\n",
    "            otimizador_sgd.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():  # Mixed Precision\n",
    "                features = modelo(entradas)  # Extrair features do ViT\n",
    "                saidas = classifier(features)  # Passar pelo classificador\n",
    "                erro = metrica_erro(saidas, labels)  # Calcular perda\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(erro).backward()\n",
    "            scaler.step(otimizador_sgd)\n",
    "            scaler.update()\n",
    "\n",
    "            erro_treino += erro.item() * entradas.size(0)\n",
    "            _, preds = torch.max(saidas, 1)\n",
    "            acuracia_treino += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Modo de avaliao\n",
    "        modelo.eval()\n",
    "        classifier.eval()\n",
    "        erro_validacao = 0.0\n",
    "        acuracia_validacao = 0.0\n",
    "\n",
    "        # Inicializar variveis para calcular a acurcia por classe\n",
    "        total_por_classe = torch.zeros(num_classes, device=device)\n",
    "        corretos_por_classe = torch.zeros(num_classes, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for entradas, labels in tqdm(data_loader_validacao, desc=\"Validando\"):\n",
    "                entradas, labels = entradas.to(device), labels.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    features = modelo(entradas)\n",
    "                    saidas = classifier(features)\n",
    "                    erro = metrica_erro(saidas, labels)\n",
    "\n",
    "                erro_validacao += erro.item() * entradas.size(0)\n",
    "                _, preds = torch.max(saidas, 1)\n",
    "                acuracia_validacao += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Atualizar contadores por classe\n",
    "                for classe in range(num_classes):\n",
    "                    total_por_classe[classe] += torch.sum(labels == classe)\n",
    "                    corretos_por_classe[classe] += torch.sum((preds == classe) & (labels == classe))\n",
    "\n",
    "        # Calcular mtricas\n",
    "        erro_medio_treino = erro_treino / len(data['treino'])\n",
    "        acuracia_medio_treino = acuracia_treino.double() / len(data['treino'])\n",
    "        erro_medio_validacao = erro_validacao / len(data['validacao'])\n",
    "        acuracia_medio_validacao = acuracia_validacao.double() / len(data['validacao'])\n",
    "\n",
    "        historico.append([erro_medio_treino, erro_medio_validacao, acuracia_medio_treino, acuracia_medio_validacao])\n",
    "\n",
    "        print(f\"Treino - Erro: {erro_medio_treino:.4f}, Acurcia: {acuracia_medio_treino:.4f}\")\n",
    "        print(f\"Validao - Erro: {erro_medio_validacao:.4f}, Acurcia: {acuracia_medio_validacao:.4f}\")\n",
    "\n",
    "        # Mostrar acurcia por classe\n",
    "        print(\"Acurcia por classe na validao:\")\n",
    "        for classe in range(num_classes):\n",
    "            taxa_acerto = (corretos_por_classe[classe] / total_por_classe[classe]).item() if total_por_classe[classe] > 0 else 0.0\n",
    "            print(f\"Classe {classe}: {taxa_acerto * 100:.2f}%\")\n",
    "\n",
    "        # Salvar checkpoints\n",
    "        salvar_checkpoint_vit(modelo, classifier, otimizador_sgd, epoca + 1, checkpoint_path)\n",
    "\n",
    "        # Atualizar o melhor modelo\n",
    "        if acuracia_medio_validacao > melhor_acuracia:\n",
    "            melhor_acuracia = acuracia_medio_validacao\n",
    "            torch.save(modelo.state_dict(), 'melhor_modelo_nosavedata01.pth')\n",
    "            torch.save(classifier.state_dict(), 'melhor_classifier_nosavedata01.pth')\n",
    "            print(\"Melhor modelo salvo!\")\n",
    "\n",
    "    return historico\n",
    "\n",
    "\n",
    "# Treinar o modelo\n",
    "historico = treinar_e_validar(vit_model, classifier, funcao_erro, otimizador, numero_de_epocas, iniciar_epoca=epoca_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Separar os dados do histrico\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m historico \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorico\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m erro_treino \u001b[38;5;241m=\u001b[39m historico[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m erro_validacao \u001b[38;5;241m=\u001b[39m historico[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separar os dados do histrico\n",
    "historico = np.array(historico)\n",
    "erro_treino = historico[:, 0]\n",
    "erro_validacao = historico[:, 1]\n",
    "acuracia_treino = historico[:, 2]\n",
    "acuracia_validacao = historico[:, 3]\n",
    "\n",
    "# Configurar as pocas\n",
    "epocas = range(1, len(erro_treino) + 1)\n",
    "\n",
    "# Plotar grfico de erro\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epocas, erro_treino, label='Erro - Treino')\n",
    "plt.plot(epocas, erro_validacao, label='Erro - Validao')\n",
    "plt.title('Erro por poca')\n",
    "plt.xlabel('poca')\n",
    "plt.ylabel('Erro')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotar grfico de acurcia\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epocas, acuracia_treino, label='Acurcia - Treino')\n",
    "plt.plot(epocas, acuracia_validacao, label='Acurcia - Validao')\n",
    "plt.title('Acurcia por poca')\n",
    "plt.xlabel('poca')\n",
    "plt.ylabel('Acurcia')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
