{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "'''MLP AND LINEARS'''\n",
    "\n",
    "def init_relu(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.orthogonal_(module.weight, gain=1.41421)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_orth(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.orthogonal_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_xavier(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_uniform_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "def init_xavier_normal(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_zeros(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.zeros_(module.weight)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def init_sigmoid(module):\n",
    "    #print(f\"The init sigmoid was only tested by the package's author at the CfC.\")\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_lecun(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=1.0 / (module.weight.shape[1])**0.5)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_tanh(module):\n",
    "    #print(f\"The init tanh was only tested by the package's author at the CfC.\")\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.xavier_normal_(module.weight, gain=1.6667)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_deep_lstm(module):\n",
    "    # Ref: Sequence to Sequence Learning with Neural Networks\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        nn.init.uniform_(module.weight, -0.08, 0.08)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_alphastar_special(module):\n",
    "    # Ref: Alphastar\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.005)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_emb(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        torch.nn.init.normal_(module.weight, std=math.sqrt(1/module.weight.shape[0]))\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias) \n",
    "            \n",
    "    if type(module) == nn.Embedding:\n",
    "        torch.nn.init.normal_(module.weight, std=math.sqrt(1/module.weight.shape[1]))\n",
    "\n",
    "def init_saving_variance(module, num_blks):\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(module.weight, gain=torch.tensor(4*num_blks).pow(-1/4))\n",
    "    if hasattr(module, 'bias'):\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "def init_gpt(module):\n",
    "    #print(f\"From init_gpt.\\nGpt proj linears should have a special weight initialization not implemented here.\")\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        #torch.nn.init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        #torch.nn.init.xavier_normal_(module.weight)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        nn.init.constant_(module.weight, 1.0)\n",
    "        \n",
    "\n",
    "def init_proj(module):\n",
    "    assert not isinstance(module, nn.Conv1d) and not isinstance(module, nn.Conv2d) and not isinstance(module, nn.Conv3d)\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.eye_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "'''CNN'''\n",
    "\n",
    "def init_cnn(module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.Conv1d or type(module) == nn.Conv3d:\n",
    "        #nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in', nonlinearity='SiLU')\n",
    "        nn.init.orthogonal_(module.weight, 1)\n",
    "        #nn.init.orthogonal_(module.weight, 1.41421)\n",
    "        #nn.init.xavier_uniform_(module.weight, 1)\n",
    "        #nn.init.xavier_uniform_(module.weight, 1.41421)\n",
    "\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def init_partial_dirac(module):\n",
    "    if type(module) in (nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        w = module.weight.data\n",
    "        \n",
    "        nn.init.dirac_(module.weight[:w.shape[1]])\n",
    "        nn.init.xavier_uniform_(module.weight[w.shape[1]:], gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    if type(module) == nn.Linear:\n",
    "        print(f\"ERROR: ONLY CONVOLUTIONS ARE SUPPORTED BY THE DIRAC INITIALIZATION.\")\n",
    "\n",
    "def init_dreamer_normal(module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.Conv1d or type(module) == nn.Conv3d:\n",
    "\n",
    "        if type(module)==nn.Linear():\n",
    "            space = module.weight.shape[1] * module.weight.shape[0]\n",
    "            in_num = space * module.weight.shape[1]\n",
    "            out_num = space * module.weight.shape[1]\n",
    "        else:\n",
    "            space = module.kernel_size[0] * module.kernel_size[1]\n",
    "            in_num = space * module.in_channels\n",
    "            out_num = space * module.out_channels\n",
    "        \n",
    "        std = np.sqrt((1/np.mean(np.array([in_num, out_num])))) / 0.87962566103423978\n",
    "        nn.init.trunc_normal_(module.weight.data, mean=0.0, std=std, a=-2.0 * std, b=2.0 * std)\n",
    "        \n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "        \n",
    "\n",
    "def init_dreamer_uniform(m):\n",
    "    # Same as xavier uniform\n",
    "    '''\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        #nn.init.orthogonal_(m.weight, 1.41421)\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        in_num = m.in_features\n",
    "        out_num = m.out_features\n",
    "        denoms = (in_num + out_num) / 2.0\n",
    "        scale = 1.0 / denoms\n",
    "        limit = np.sqrt(3 * scale)\n",
    "        nn.init.uniform_(m.weight.data, a=-limit, b=limit)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "    \n",
    "\n",
    "    \n",
    "def init_proj2d(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        torch.nn.init.dirac_(module.weight, groups=1)\n",
    "        \n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "'''WHITENED LAYERS'''\n",
    "\n",
    "def get_patches(x, patch_shape):\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    \n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    n,c,h,w = patches.shape\n",
    "    patches_flat = patches.view(n, -1)\n",
    "    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
    "    \n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "def init_whitening_conv(layer, train_set, eps=5e-4):\n",
    "    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n",
    "    \n",
    "    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n",
    "    \n",
    "    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n",
    "    \n",
    "    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "    layer.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCES\n",
    "# https://github.com/karpathy/nanoGPT\n",
    "# https://github.com/JegZheng/truncated-diffusion-probabilistic-models\n",
    "# https://github.com/facebookresearch/DiT/blob/main/models.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script # JIT decorator\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "from torch import nn\n",
    "import inspect\n",
    "def network_ema(target_network, new_network, alpha=0.5):\n",
    "    for (param_name, param_target), param_new  in zip(target_network.cuda().named_parameters(), new_network.parameters()):\n",
    "        if 'ln' in param_name: #layer norm\n",
    "            param_target.data = param_new.data.clone()\n",
    "        else:\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param_new.data.clone()\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def params_count(model, name='Model'):\n",
    "    params_to_count = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f'{name} Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "\n",
    "def params_and_grad_norm(model):\n",
    "    param_norm, grad_norm = 0, 0\n",
    "    for n, param in model.named_parameters():\n",
    "        if not n.endswith('.bias'):\n",
    "            param_norm += torch.norm(param.data)\n",
    "            if param.grad is not None:\n",
    "                grad_norm += torch.norm(param.grad)\n",
    "    return param_norm, grad_norm\n",
    "\n",
    "\n",
    "# From STORM Atari-100k\n",
    "def seed_np_torch(seed=20001118):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "\n",
    "def statistical_difference(p1, p2, n):\n",
    "    # order invariant\n",
    "    \n",
    "    d=torch.tensor(p1-p2).abs()\n",
    "    std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "    difference = torch.tensor([d-std, d+std])\n",
    "        \n",
    "    difference = difference.sort()[0]\n",
    "    \n",
    "    return difference\n",
    "\n",
    "def renormalize(tensor):\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.view(shape[0], -1)\n",
    "    max_value,_ = torch.max(tensor, -1, keepdim=True)\n",
    "    min_value,_ = torch.min(tensor, -1, keepdim=True)\n",
    "    return ((tensor - min_value) / (max_value - min_value + 1e-5)).view(shape)\n",
    "\n",
    "# Hyper Parameters\n",
    "# automatically saves all arguments of the inherited class __init__\n",
    "class Hypers: # Sorcery\n",
    "    def __init__(self, max_depth=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hypers(max_depth)\n",
    "    \n",
    "    def save_hypers(self, max_depth, ignore=[]):\n",
    "      \"\"\"Save function arguments into class attributes.\"\"\"\n",
    "\n",
    "      #f_back: frame caller\n",
    "      #frame: table of local variablies to the frame's function\n",
    "      seen_init=False\n",
    "      frame = inspect.currentframe()\n",
    "      for d in range(max_depth):\n",
    "          \n",
    "          frame = frame.f_back\n",
    "          \n",
    "          if frame.f_back and frame.f_back.f_code.co_name == \"__init__\":\n",
    "              seen_init=True\n",
    "              \n",
    "          if seen_init and frame.f_back.f_code.co_name != \"__init__\":\n",
    "              break\n",
    "            \n",
    "      _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "      #takes the arguments of the function which called this save_hypers function\n",
    "      #it can backtrack functions according to the depth argument\n",
    "\n",
    "      self.hparams = {k:v for k, v in local_vars.items()\n",
    "          if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "      for k, v in self.hparams.items():\n",
    "          setattr(self, k, v)\n",
    "\n",
    "\n",
    "# ALLWAYS PUT HYPERS TO THE LEFT\n",
    "class nsd_Module(Hypers, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(max_depth=3)\n",
    "class FusedGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return fused_gelu(x)\n",
    "\n",
    "\n",
    "class LayerNormNoBias(nn.Module):\n",
    "    \"\"\" LayerNormNoBias but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "    \n",
    "class Attention(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1, seq_len=8):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.k_pre = None\n",
    "        self.k_post = None\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    def forward_xl(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "\n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        self.k_pre = k.detach()\n",
    "        self.v_pre = v.detach()\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "    def forward_xl_windowed(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        if self.k_pre == None:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        elif self.k_pre.shape[-2] < self.seq_len:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        else:\n",
    "            self.k_pre = k[...,1:,:].detach()\n",
    "            self.v_pre = v[...,1:,:].detach()\n",
    "\n",
    "\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.k_post,k),-2)\n",
    "            v = torch.cat((self.v_post,v),-2)\n",
    "        \n",
    "\n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class Attention_XL(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "\n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        self.k_pre = k.detach()\n",
    "        self.v_pre = v.detach()\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class Attention_XL_window(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1, seq_len=8):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, q, k, v, is_causal):\n",
    "        B, T, C = q.size()\n",
    "        \n",
    "        q = self.W_k(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        if self.k_pre == None:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        elif self.k_pre.shape[-2] < self.seq_len:\n",
    "            self.k_pre = k.detach()\n",
    "            self.v_pre = v.detach()\n",
    "        else:\n",
    "            self.k_pre = k[...,1:,:].detach()\n",
    "            self.v_pre = v[...,1:,:].detach()\n",
    "\n",
    "\n",
    "        if self.k_post!=None:\n",
    "            k = torch.cat((self.post,k),-2)\n",
    "            v = torch.cat((self.post,v),-2)\n",
    "        \n",
    "\n",
    "        self.k_post = self.k_pre\n",
    "        self.v_post = self.v_pre\n",
    "\n",
    "\n",
    "\n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MemoryAttention(nsd_Module):\n",
    "    def __init__(self, d_model=512, nhead=8, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.W_kv = nn.Linear(d_model, 2 * d_model, bias=bias)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, q):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k, v  = self.W_kv(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        \n",
    "        # FoT LongLlama contrastive style (data pipeline constrastive for self attention enrichment)\n",
    "        \n",
    "        shifted_k=[]\n",
    "        shifted_v=[]\n",
    "        for i in range(7): # 7 is d-1 for d=8\n",
    "            shifted_k.append(torch.roll(k[:,:T//2],i,0))\n",
    "            shifted_v.append(torch.roll(v[:,:T//2],i,0))\n",
    "        shifted_k=torch.stack(shifted_k).view(B,-1,C)\n",
    "        shifted_v=torch.stack(shifted_v).view(B,-1,C)\n",
    "        \n",
    "        k=torch.concat((shifted_k,k),1)\n",
    "        v=torch.concat((shifted_v,v),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        L = q.shape[2]\n",
    "        S = k.shape[2]\n",
    "        attn_mask = torch.ones(L, S, dtype=torch.bool, device='cuda').tril(diagonal=S-L)\n",
    "        attn_mask[:T//2,:S-L]=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "            #y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, dropout_p=self.dropout if self.training else 0)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    def forward_memory(self, x, q, k_read, v_read):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        \n",
    "        k, v = self.W_kv(x).split(self.n_embd, dim=2)\n",
    "        write_k, write_v = k.detach(), v.detach()\n",
    "        \n",
    "        k=torch.cat((k_read, k), 1)\n",
    "        v=torch.cat((v_read, v), 1)\n",
    "        \n",
    "        #shifted_k=[]\n",
    "        #shifted_v=[]\n",
    "        #for i in range(7): # 7 is d-1 for d=8\n",
    "        #    shifted_k.append(torch.roll(k[:,:T//2],i,0))\n",
    "        #    shifted_v.append(torch.roll(v[:,:T//2],i,0))\n",
    "        #shifted_k=torch.stack(shifted_k).view(B,-1,C)\n",
    "        #shifted_v=torch.stack(shifted_v).view(B,-1,C)\n",
    "        \n",
    "        #k=torch.cat((shifted_k, k), 1)\n",
    "        #v=torch.cat((shifted_v, v), 1)\n",
    "        \n",
    "        q = q.view(B, T, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k_read = k_read.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2)\n",
    "        v_read = v_read.view(B, -1, self.nhead, C // self.nhead).transpose(1, 2)\n",
    "          \n",
    "        \n",
    "        # Causal Mask\n",
    "        L = q.shape[2]\n",
    "        S = k.shape[2]-q.shape[2]\n",
    "        causal_mask = torch.ones(L, L, dtype=torch.bool, device='cuda').tril(diagonal=0)\n",
    "        eye_mask=torch.eye(L, dtype=torch.bool, device='cuda')\n",
    "        read_attnmask=torch.ones(L, L*3, dtype=torch.bool, device='cuda')\n",
    "        aux=torch.arange(L).repeat_interleave(3)\n",
    "        \n",
    "        #new_attnmask=causal_mask[:,aux]\n",
    "        read_attnmask=eye_mask[:,aux]\n",
    "        \n",
    "        attn_mask=torch.cat((read_attnmask,causal_mask),1)\n",
    "        \n",
    "        #shift_mask = torch.ones(L, int(L*3.5), dtype=torch.bool, device='cuda')\n",
    "        #shift_mask[:T//2,:]=False\n",
    "        #attn_mask=torch.cat((shift_mask,attn_mask),1)\n",
    "        \n",
    "        \n",
    "        # Memory Mask\n",
    "        memory_mask = torch.ones(L*3, L, dtype=torch.bool, device='cuda')\n",
    "        memory_mask=torch.concat((torch.eye(L*3, dtype=torch.bool, device='cuda'), memory_mask),1)\n",
    "        #memory_mask=torch.concat((~torch.ones(L*3, int(L*3.5), dtype=torch.bool, device='cuda'), memory_mask),1)\n",
    "        \n",
    "        # Associative Learning\n",
    "        std=0.5\n",
    "        noise=torch.randn_like(k_read)*std\n",
    "        k_read=F.normalize(k_read)\n",
    "        k_read=k_read+noise\n",
    "        \n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            y = F.scaled_dot_product_attention(q,k,v,attn_mask=attn_mask,\n",
    "                                                dropout_p=self.dropout)\n",
    "            v_read = F.scaled_dot_product_attention(k_read,k,v, attn_mask=memory_mask,\n",
    "                                                    dropout_p=0)\n",
    "            k_read = F.scaled_dot_product_attention(k_read,k,k, attn_mask=memory_mask,\n",
    "                                                    dropout_p=0)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        k_read = k_read.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        v_read = v_read.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        \n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.proj(y))\n",
    "        return y, write_k, write_v, k_read, v_read\n",
    "        #return y, write_k, write_v, None,None\n",
    "\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout=0.1, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.fc    = nn.Linear(d_model, ffn_mult * d_model, bias=bias)\n",
    "        self.gelu  = nn.GELU()\n",
    "        self.proj  = nn.Linear(ffn_mult * d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPT_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, seq_len=8):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout, seq_len)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward_xl_windowed(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn.forward_xl_windowed(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x    \n",
    "    \n",
    "\n",
    "\n",
    "class GPT_Transformer(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_encoding = nn.Sequential(nn.Linear(seq_len, d_model, bias=False),\n",
    "        #                                  LayerNormNoBias(d_model)) #Stable Embedding Layer # Requires One Hot\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), GPT_Block(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult, seq_len=seq_len))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "        \n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "\n",
    "    def forward_xl_windowed(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk.forward_xl_windowed(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)    \n",
    "\n",
    "\n",
    "class GPT_NLP(nsd_Module):\n",
    "    def __init__(self, hiddens, num_blks, nhead, seq_len, vocab_size=50257,\n",
    "                 temperature=1.0, k=20, p=0.9, sampling='gpt', report_params_count=True, tied_weights=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.emb_vocab = nn.Embedding(vocab_size, hiddens)\n",
    "        self.gpt = GPT_Transformer(hiddens, nhead=nhead, num_blks=num_blks)\n",
    "        \n",
    "        self.cls = nn.Linear(hiddens, vocab_size, bias=False)\n",
    "        \n",
    "        if tied_weights:\n",
    "            self.emb_vocab.weight = self.cls.weight\n",
    "\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT NLP Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "    def forward(self, X, is_causal=True):\n",
    "        batch_size, seq_len = X.shape\n",
    "        \n",
    "        mask = X>self.vocab_size\n",
    "        X[mask] = self.vocab_size-1\n",
    "        \n",
    "        X = self.emb_vocab(X)\n",
    "        #cls = torch.autograd.Variable(torch.zeros(batch_size, 2, self.hiddens)).to('cuda')\n",
    "        \n",
    "        #X = torch.cat((X, cls), dim=1)\n",
    "        X = self.gpt(X, is_causal=is_causal)\n",
    "\n",
    "        return self.cls(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPT_Block_XL(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, seq_len=8, windowed=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        if windowed:\n",
    "            self.attn = Attention_XL_window(d_model, nhead, bias, dropout, seq_len=seq_len)\n",
    "        else:\n",
    "            self.attn = Attention_XL(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class GPT_Transformer_XL(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, windowed=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_encoding = nn.Sequential(nn.Linear(seq_len, d_model, bias=False),\n",
    "        #                                  LayerNormNoBias(d_model)) #Stable Embedding Layer # Requires One Hot\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), GPT_Block_XL(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult, seq_len=seq_len, windowed=windowed))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "        \n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer_Block_NoLN(nsd_Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4, stochastic_depth=1):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        #x = renormalize(x)\n",
    "        keep_path = torch.ones(x.shape[0],device='cuda')*(self.stochastic_depth if self.training else 1)\n",
    "        keep_path = torch.bernoulli(keep_path)[:,None,None]\n",
    "\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=is_causal)*keep_path\n",
    "        \n",
    "        x = x + self.mlp(self.ln_2(x))*keep_path\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer_NoDATA(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0, scale_init=1):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = d_model\n",
    "        self.scale_init=scale_init\n",
    "        if scale_init==1:\n",
    "            self.scale_init=num_blks\n",
    "\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "\n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "        self.num_blks=num_blks\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), Transformer_Block_NoLN(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult,\n",
    "                                stochastic_depth=1-((1-stochastic_depth)*i/num_blks) ))\n",
    "\n",
    "\n",
    "        # https://proceedings.mlr.press/v119/huang20f/huang20f.pdf\n",
    "\n",
    "        #self.apply(init_gpt)\n",
    "        #for pn, p in self.named_parameters():\n",
    "        #    if pn.endswith('proj.weight'):\n",
    "        #        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "\n",
    "        self.apply(init_xavier)\n",
    "        \n",
    "        #for pn, p in self.named_parameters():\n",
    "        #    if pn.endswith('proj.weight') or pn.endswith('W_v.weight') or pn.endswith('fc.weight') or pn.endswith('pos_encoding.weight'):\n",
    "        #        torch.nn.init.xavier_uniform_(p, gain=(torch.tensor(4*self.scale_init,dtype=torch.float)).pow(-1/4))\n",
    "        #self.apply(self._init_weights)\n",
    "        \n",
    "\n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=1/math.sqrt(self.num_hiddens))\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=(torch.tensor(4*self.scale_init,dtype=torch.float)).pow(-1/4))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        \n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def no_pos(self, X, is_causal=True):\n",
    "        X = self.start_dropout(X)\n",
    "        \n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "\n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def masked(self, X, mask, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)[:X.shape[1]]\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        X = X.gather(1, mask)\n",
    "        \n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "\n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def modulate(x, shift, scale):\n",
    "    # x (B, T, D)\n",
    "    # shift (B, D)\n",
    "    # scale (B, D)\n",
    "    \n",
    "    return x * (1 + scale[:,None]) + shift[:,None]\n",
    "\n",
    "\n",
    "class DiT_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "        \n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, 6 * d_model, bias=True)\n",
    "        )\n",
    "\n",
    "        self.ln_1.apply(init_gpt)\n",
    "        self.attn.apply(init_gpt)\n",
    "        self.ln_2.apply(init_gpt)\n",
    "        self.mlp.apply(init_gpt)\n",
    "        self.adaLN_modulation.apply(init_zeros)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        \n",
    "        x_ln = modulate(self.ln_1(x), shift_msa, scale_msa)\n",
    "        \n",
    "        x = x + (1+gate_msa[:,None]) * self.attn(x_ln, x_ln, x_ln, is_causal=False)\n",
    "        x = x + (1+gate_mlp[:,None]) * self.mlp(modulate(self.ln_2(x), shift_mlp, scale_mlp))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward_no_dit(self, x):\n",
    "        x_ln = self.ln_1(x)\n",
    "        x = x + self.attn(x_ln, x_ln, x_ln, is_causal=False)\n",
    "        return x + self.mlp(self.ln_2(x))\n",
    "    \n",
    "    \n",
    "class DiT_Transformer(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, scale_init=1):\n",
    "        super().__init__()\n",
    "        if scale_init==1:\n",
    "            scale_init=num_blks\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), DiT_Block(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(init_gpt)\n",
    "        self.init_weights()\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "\n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \n",
    "        # Zero-out adaLN modulation layers in DiT blocks:\n",
    "        for block in self.blks:\n",
    "            block.adaLN_modulation[-1].apply(init_zeros)\n",
    "    \n",
    "        \n",
    "    def forward(self, X, c):\n",
    "        # Input:\n",
    "        # X e (B, T, D)\n",
    "        # c e (B, D)\n",
    "        \n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        \n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, c)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "    \n",
    "\n",
    "    def forward_no_dit(self, X):\n",
    "        # Input:\n",
    "        # X e (B, T, D)\n",
    "        # c e (B, D)\n",
    "        \n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        \n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk.forward_no_dit(X)\n",
    "            \n",
    "        return self.final_ln(X)\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "class CrossAttention_Block(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, bias=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, nhead, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias)\n",
    "\n",
    "    def forward(self, q, k, v, is_causal=False):\n",
    "        q = q + self.attn(self.ln_1(q),self.ln_1(k),self.ln_1(v), is_causal=is_causal)\n",
    "        q = q + self.mlp(self.ln_2(q))\n",
    "        return q\n",
    "    \n",
    "\n",
    "\n",
    "class CrossAttention_Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len, dim_feedforward=2048,  \n",
    "                 dropout = 0.1, vocab_size = 0, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.out_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), CrossAttention_Block(\n",
    "                                d_model, nhead, dropout, bias=False))\n",
    "            \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "\n",
    "\n",
    "        \n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "    \n",
    "    def forward(self, q, k, v, is_causal=False):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.long, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        q = self.start_dropout(q+pos_emb)\n",
    "        k = self.start_dropout(k+pos_emb)\n",
    "        v = self.start_dropout(v+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            q = blk.forward(q,k,v, is_causal)\n",
    "        q = self.out_ln(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SpatialNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.\n",
    "\n",
    "    Args:\n",
    "        f_channels (`int`):\n",
    "            The number of channels for input to group normalization layer, and output of the spatial norm layer.\n",
    "        zq_channels (`int`):\n",
    "            The number of channels for the quantized vector as described in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f_channels: int,\n",
    "        zq_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-6, affine=True)\n",
    "        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, f: torch.FloatTensor, zq: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        f_size = f.shape[-2:]\n",
    "        zq = F.interpolate(zq, size=f_size, mode=\"nearest\")\n",
    "        norm_f = self.norm_layer(f)\n",
    "        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)\n",
    "        return new_f\n",
    "    \n",
    "    \n",
    "    \n",
    "class ConvAttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels, t_emb_dim=512, dropout=0, nhead=8):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.dropout = dropout\n",
    "        self.nhead = in_channels//nhead\n",
    "        \n",
    "        self.norm = nn.GroupNorm(32, in_channels)\n",
    "        \n",
    "        #self.norm = SpatialNorm(in_channels, t_emb_dim)\n",
    "\n",
    "        self.q = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.k = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.v = torch.nn.Linear(in_channels,\n",
    "                                 in_channels)\n",
    "        self.proj_out = torch.nn.Linear(in_channels,\n",
    "                                        in_channels)\n",
    "        self.q.apply(init_cnn)\n",
    "        self.k.apply(init_cnn)\n",
    "        self.v.apply(init_cnn)\n",
    "        self.proj_out.apply(init_cnn)\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_).view(b, c, h*w).transpose(1,2)\n",
    "        \n",
    "        #h_ = self.norm(h_, t_emb)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        q = q.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "        k = k.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "        v = k.contiguous().view(b, h*w, self.nhead, c//self.nhead).transpose(1, 2)\n",
    "\n",
    "        # compute attention\n",
    "\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            h_ = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "\n",
    "        h_ = h_.transpose(1, 2).view(b, h*w, c)\n",
    "        h_ = self.proj_out(h_).transpose(1,2)\n",
    "\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "    \"\"\"\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        print(f\"{h_.shape}\")\n",
    "        #h_ = self.norm(h_, t_emb)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b, c, h, w = q.shape\n",
    "        q = q.view(b, c, h*w).transpose(1,2)\n",
    "        k = k.view(b, c, h*w).transpose(1,2)\n",
    "        v = v.view(b, c, h*w).transpose(1,2)\n",
    "        '''\n",
    "        w_ = torch.bmm(q, k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        w_ = w_.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
    "        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = torch.bmm(v, w_)\n",
    "        '''\n",
    "        with torch.backends.cuda.sdp_kernel():\n",
    "            h_ = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)\n",
    "\n",
    "        h_ = h_.transpose(1, 2)\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_hiddens=512, med_hiddens=512, out_hiddens=512, layers=1,\n",
    "                 init=init_relu, in_act=nn.SiLU(), out_act=nn.Identity(),\n",
    "                 ln_eps=1e-3, last_init=init_xavier, bias=True):\n",
    "        super().__init__()\n",
    "        # Special MLP with custom options for non last layer and last layer Linears.\n",
    "\n",
    "        modules=[]\n",
    "        self.init=init\n",
    "        self.last_init=last_init\n",
    "        \n",
    "        hiddens=in_hiddens\n",
    "        _out_hiddens = med_hiddens\n",
    "        act = in_act\n",
    "        for l in range(layers):\n",
    "            last_layer = l==(layers-1)\n",
    "            if last_layer:\n",
    "                _out_hiddens = out_hiddens\n",
    "                act = out_act\n",
    "            modules.append(nn.Linear(hiddens, _out_hiddens, bias=bias))\n",
    "            \n",
    "            modules.append(act)\n",
    "            hiddens=med_hiddens\n",
    "        self.mlp=nn.Sequential(*modules)\n",
    "        #print(self.mlp)\n",
    "\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def turn_off_grads(self):\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                layer.weight.requires_grad=False\n",
    "            if hasattr(layer, 'bias'):\n",
    "                layer.bias.requires_grad=False\n",
    "    def init_weights(self):\n",
    "        self.mlp.apply(self.init)\n",
    "        self.mlp[-2].apply(self.last_init)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.mlp(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class ViT(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, patches=(16,16), img_size=(96,72), first_channel=3,\n",
    "                 dropout = 0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = np.prod(patches)\n",
    "        self.N = int(np.prod(img_size)/self.patches)\n",
    "\n",
    "        self.in_proj = MLP(first_channel*self.patches, out_hiddens=d_model, last_init=init_gpt)\n",
    "\n",
    "        self.cls = nn.Embedding(1,d_model)\n",
    "        self.transformer = Transformer_NoDATA(d_model, num_blks, nhead, seq_len=self.N,\n",
    "                 dropout = dropout, bias=bias, report_params_count=False,\n",
    "                 ffn_mult=ffn_mult, stochastic_depth=stochastic_depth)\n",
    "\n",
    "        self.cls.apply(init_gpt)\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'ViT')\n",
    "\n",
    "    def patchify(self, X):\n",
    "        X = X.view(-1, self.patches*self.first_channel, self.N).transpose(-2,-1)\n",
    "        return X\n",
    "\n",
    "    def proj(self, X):\n",
    "        X = self.patchify(X)\n",
    "        return self.in_proj(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patchify(X)\n",
    "        X = self.in_proj(X)\n",
    "\n",
    "        X = self.transformer(X, is_causal=False)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def masked(self, X, mask):\n",
    "        \n",
    "        X = self.transformer.masked(X, mask, is_causal=False)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "class ViT(nsd_Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, patches=(16,16), img_size=(96,72), first_channel=3,\n",
    "                 dropout=0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = np.prod(patches)\n",
    "        self.N = int(np.prod(img_size) / self.patches)\n",
    "\n",
    "        self.in_proj = MLP(first_channel * self.patches, out_hiddens=d_model, last_init=init_gpt)\n",
    "\n",
    "        # Classe \"token\" de pooling\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.transformer = Transformer_NoDATA(\n",
    "            d_model, num_blks, nhead, seq_len=self.N + 1,\n",
    "            dropout=dropout, bias=bias, report_params_count=False,\n",
    "            ffn_mult=ffn_mult, stochastic_depth=stochastic_depth\n",
    "        )\n",
    "\n",
    "        # Inicializar pesos do CLS token\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'ViT')\n",
    "\n",
    "    def patchify(self, X):\n",
    "        # Dividir a imagem em patches e reformatar\n",
    "        X = X.view(-1, self.patches * self.first_channel, self.N).transpose(-2, -1)\n",
    "        return X\n",
    "\n",
    "    def proj(self, X):\n",
    "        X = self.patchify(X)\n",
    "        return self.in_proj(X)\n",
    "    \n",
    "    def transformers(self, X):\n",
    "        \n",
    "        X = self.transformer(X, is_causal=False).view(-1, self.stacked_frames*self.N, self.d_model)\n",
    "        X = self.temporal_aggr(X, is_causal=False)\n",
    "        \n",
    "        return X[:,-self.N:]\n",
    "\n",
    "    def masked(self, X, mask):\n",
    "        \n",
    "        X = self.transformer.masked(X, mask, is_causal=False).view(-1, self.stacked_frames*mask.shape[1], self.d_model)\n",
    "        X = self.temporal_aggr(X, is_causal=False)\n",
    "        \n",
    "        return X[:,-mask.shape[1]:]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Criar patches e projetar\n",
    "        X = self.patchify(X)\n",
    "        X = self.in_proj(X)\n",
    "\n",
    "        # Adicionar o token de classe\n",
    "        batch_size = X.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        X = torch.cat((cls_tokens, X), dim=1)\n",
    "\n",
    "        # Passar pelo transformer\n",
    "        X = self.transformer(X, is_causal=False)\n",
    "\n",
    "        # Retornar apenas o token de classe\n",
    "        return X[:, 0]  # Forma: [batch_size, d_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViT_IWM(nsd_Module):\n",
    "    def __init__(self, encoder,\n",
    "                 d_predictor, num_blks_predictor, nhead_predictor,\n",
    "                 stacked_frames=4,\n",
    "                 mask_samples=4,\n",
    "                 masked_tokens=4,\n",
    "                 num_augmentations=3,\n",
    "                 first_channel=3,\n",
    "                 dropout = 0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, stochastic_depth=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_encoder = encoder.d_model\n",
    "        \n",
    "        \n",
    "        self.first_channel = encoder.first_channel*stacked_frames\n",
    "        self.img_size = encoder.img_size\n",
    "        self.patches = encoder.patches\n",
    "        self.N = encoder.N\n",
    "        self.masked_tokens=self.N//masked_tokens\n",
    "\n",
    "        # Mask\n",
    "        self.mask = MLP(1, out_hiddens=d_predictor, last_init=init_xavier)\n",
    "        self.mask_pos_encoding = nn.Embedding(self.N, d_predictor)\n",
    "        self.mask_mlp = MLP(d_predictor+num_augmentations, d_predictor, d_predictor, layers=4, in_act=nn.ReLU(), out_act=nn.ReLU(),\n",
    "                            init=init_relu, last_init=init_gpt)\n",
    "        self.mask_pos_encoding.apply(init_gpt)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Predictor\n",
    "        self.predictor_proj = MLP(self.d_encoder, out_hiddens=d_predictor, last_init=init_gpt) \\\n",
    "                              if d_predictor!=self.d_encoder else nn.Identity()\n",
    "\n",
    "        self.predictor = Transformer_NoDATA(d_predictor, num_blks_predictor, nhead_predictor, seq_len=self.N+1,\n",
    "                 dropout = dropout, bias=bias, report_params_count=False,\n",
    "                 ffn_mult=ffn_mult, scale_init=num_blks_predictor, stochastic_depth=stochastic_depth)\n",
    "\n",
    "\n",
    "        self.predictor_out_proj = MLP(d_predictor, out_hiddens=self.d_encoder, last_init=init_gpt) \\\n",
    "                              if d_predictor!=self.d_encoder else nn.Identity()\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'IWM')\n",
    "\n",
    "    def hard_reset(self, new_network, alpha):\n",
    "        network_ema(self.encoder, new_network.encoder, alpha)\n",
    "\n",
    "        network_ema(self.predictor_proj, new_network.predictor_proj, alpha)\n",
    "        network_ema(self.predictor, new_network.predictor, alpha)\n",
    "\n",
    "        network_ema(self.mask, new_network.mask, alpha)\n",
    "        network_ema(self.mask_pos_encoding, new_network.mask_pos_encoding, alpha)\n",
    "        network_ema(self.mask_mlp, new_network.mask_mlp, alpha)\n",
    "\n",
    "    def get_random_mask(self, X, augmentations):\n",
    "        B, T, D = X.shape\n",
    "        B = B//self.stacked_frames\n",
    "        m_rand = self.mask_samples*random.randint(0,int(self.masked_tokens*2//self.mask_samples)-1)\n",
    "        \n",
    "        \n",
    "        # Get non-overlapping mask\n",
    "        mask_pos = torch.arange(T, device='cuda')[None,:].repeat_interleave(B,0).float()\n",
    "        mask_pos = torch.multinomial(mask_pos, num_samples=self.masked_tokens+m_rand, replacement=False)\n",
    "        \n",
    "        mask_pos_repeat = mask_pos.repeat_interleave(self.stacked_frames,0)\n",
    "\n",
    "        # Get the mask complement\n",
    "        full_range = torch.arange(T,device='cuda')[None,:].repeat_interleave(B,0)\n",
    "\n",
    "        complement = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "        complement.scatter_(1, mask_pos, 1)\n",
    "\n",
    "        complement = full_range[~complement].view(mask_pos.shape[0], -1)\n",
    "        \n",
    "\n",
    "        # Mask mlp for geometric + augmentation informations\n",
    "        mask = self.mask(torch.ones(B*self.stacked_frames,self.masked_tokens+m_rand,1, device='cuda'))\n",
    "\n",
    "        mask = mask + self.mask_pos_encoding(mask_pos_repeat)\n",
    "\n",
    "        augmentations = augmentations.repeat_interleave(self.stacked_frames,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "\n",
    "        mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "\n",
    "        # Expand to allow gather\n",
    "        mask_pos = mask_pos[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "        complement = complement[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "\n",
    "        return X, mask_pos, complement, mask\n",
    "        \n",
    "    def patchify(self, X):\n",
    "        X = X.view(-1, self.patches*self.first_channel, self.N).transpose(-2,-1)\n",
    "        return X\n",
    "    def get_block_mask(self, batch_size):\n",
    "        \n",
    "        all_wins = torch.zeros(self.first_channel,*self.img_size).long()\n",
    "        \n",
    "        b_mask, b_complement = [], []\n",
    "        min_c_len = 999 # for trunked collate\n",
    "        #min_m=999\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            wins, complements = [], []\n",
    "            for m in range(self.mask_samples):\n",
    "                w,h = self.img_size\n",
    "\n",
    "\n",
    "                min_ar, max_ar = (0.75, 1.5)\n",
    "                aspect_ratio = min_ar + random.random() * (max_ar - min_ar)\n",
    "\n",
    "                h_sample_size = int( (h*(torch.tensor(random.random())*0.05+0.15)) * aspect_ratio)\n",
    "\n",
    "                w_wins, h_wins = torch.randint(0,h-h_sample_size,(2,)).split(1,0)\n",
    "                win=all_wins.clone()\n",
    "\n",
    "\n",
    "                for w_win, h_win in zip(w_wins, h_wins):\n",
    "                    win[...,w_win:w_win+h_sample_size, h_win:h_win+h_sample_size]=1\n",
    "\n",
    "                \n",
    "                win = self.patchify(win.float()).mean(-1)\n",
    "                \n",
    "                values, idx = win.sort(descending=True)\n",
    "\n",
    "                idx = idx[:,:self.N//4]\n",
    "                \n",
    "                #min_m = min(min_m, len(values[0].nonzero()))\n",
    "                wins.append(idx)\n",
    "\n",
    "\n",
    "            wins = torch.stack(wins).squeeze()\n",
    "\n",
    "\n",
    "            full_range = torch.arange(win.shape[1])\n",
    "\n",
    "            complement = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "            complement.scatter_(0, wins.view(-1).unique(), 1)\n",
    "\n",
    "            complement = full_range[~complement]\n",
    "            min_c_len = min(min_c_len, len(complement))\n",
    "            \n",
    "            \n",
    "            b_mask.append(wins)\n",
    "            b_complement.append(complement)\n",
    "            \n",
    "            \n",
    "        for i in range(len(b_complement)):\n",
    "            b_complement[i] = b_complement[i][:min_c_len]\n",
    "        \n",
    "        b_mask = torch.stack(b_mask).cuda()\n",
    "        b_complement = torch.stack(b_complement).cuda()\n",
    "        #print(min_m)\n",
    "        \n",
    "        return b_mask, b_complement\n",
    "    \n",
    "    def get_mask(self, X, augmentations):\n",
    "        B = X.shape[0]//self.stacked_frames\n",
    "\n",
    "        \n",
    "        mask_pos, complement = self.get_block_mask(B)\n",
    "        mask_pos = mask_pos.view(B*self.mask_samples,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mask = self.mask(torch.ones(B*self.mask_samples,1,1, device='cuda'))\n",
    "        \n",
    "        mask = mask + self.mask_pos_encoding(mask_pos)\n",
    "        #augmentations = augmentations.repeat_interleave(self.stacked_frames*self.mask_samples,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "        #mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "\n",
    "\n",
    "        mask_pos = mask_pos[...,None].expand(-1,-1,self.d_encoder)\n",
    "        complement = complement[...,None].expand(-1,-1,self.d_encoder).repeat_interleave(self.stacked_frames,0)\n",
    "        \n",
    "        return mask_pos, mask, complement\n",
    "    \n",
    "    def encode(self, X):\n",
    "        return self.encoder(X)\n",
    "\n",
    "\n",
    "    def forward(self, X, y, augmentations):\n",
    "        X = self.encoder.proj(X)\n",
    "        \n",
    "        mask_pos, mask, complement = self.get_mask(X, augmentations)\n",
    "        \n",
    "        X = self.encoder.masked(X, complement)\n",
    "        X = self.predictor_proj(X)\n",
    "\n",
    "        X = torch.cat((X.repeat_interleave(4,0),mask),1)\n",
    "        \n",
    "        X = self.predictor.no_pos(X)[:,-mask.shape[1]:]\n",
    "        X = self.predictor_out_proj(X)\n",
    "        \n",
    "        return X, y.repeat_interleave(4,0).gather(1,mask_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_checkpoint_vit(modelo, classifier, otimizador, epoca, caminho):\n",
    "    checkpoint = {\n",
    "        'modelo': modelo.state_dict(),\n",
    "        'classifier': classifier.state_dict(),\n",
    "        'otimizador': otimizador.state_dict(),\n",
    "        'epoca': epoca\n",
    "    }\n",
    "    torch.save(checkpoint, caminho)\n",
    "    print(f\"Checkpoint salvo: {caminho}\")\n",
    "\n",
    "\n",
    "# Função para carregar checkpoints\n",
    "def carregar_checkpoint_vit(modelo, classifier, otimizador, caminho):\n",
    "    if os.path.exists(caminho):\n",
    "        checkpoint = torch.load(caminho)\n",
    "        modelo.load_state_dict(checkpoint['modelo'])\n",
    "        classifier.load_state_dict(checkpoint['classifier'])\n",
    "        otimizador.load_state_dict(checkpoint['otimizador'])\n",
    "        epoca_inicial = checkpoint['epoca']\n",
    "        print(f\"Checkpoint carregado: {caminho} (Época {epoca_inicial})\")\n",
    "        return epoca_inicial\n",
    "    else:\n",
    "        print(f\"Nenhum checkpoint encontrado em: {caminho}. Começando do zero.\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cuda:0\n",
      "ViT Parameters: 38.17M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15900\\1993890487.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(caminho)\n",
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15900\\391232832.py:94: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Inicializar GradScaler para Mixed Precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint carregado: ./checkpoints\\ultimo_checkpoint_v3.pth (Época 203)\n",
      "\n",
      "Época 204/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando:   0%|          | 0/449 [00:00<?, ?it/s]C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15900\\391232832.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Mixed Precision\n",
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15900\\3849317490.py:168: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n",
      "Treinando: 100%|██████████| 449/449 [06:46<00:00,  1.10it/s]\n",
      "Validando:   0%|          | 0/57 [00:00<?, ?it/s]C:\\Users\\Danil\\AppData\\Local\\Temp\\ipykernel_15900\\391232832.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3385, Acurácia: 0.4804\n",
      "Validação - Erro: 1.5317, Acurácia: 0.4386\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 23.34%\n",
      "Classe 1: 19.64%\n",
      "Classe 2: 25.81%\n",
      "Classe 3: 66.26%\n",
      "Classe 4: 46.29%\n",
      "Classe 5: 32.31%\n",
      "Classe 6: 58.07%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 205/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:44<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3383, Acurácia: 0.4833\n",
      "Validação - Erro: 1.5400, Acurácia: 0.4492\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.99%\n",
      "Classe 1: 17.86%\n",
      "Classe 2: 19.96%\n",
      "Classe 3: 70.50%\n",
      "Classe 4: 45.80%\n",
      "Classe 5: 41.04%\n",
      "Classe 6: 54.94%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 206/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3308, Acurácia: 0.4870\n",
      "Validação - Erro: 1.5317, Acurácia: 0.4455\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.63%\n",
      "Classe 1: 17.86%\n",
      "Classe 2: 19.56%\n",
      "Classe 3: 72.40%\n",
      "Classe 4: 39.37%\n",
      "Classe 5: 36.29%\n",
      "Classe 6: 64.34%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 207/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3295, Acurácia: 0.4845\n",
      "Validação - Erro: 1.5324, Acurácia: 0.4372\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.77%\n",
      "Classe 1: 23.21%\n",
      "Classe 2: 22.58%\n",
      "Classe 3: 65.59%\n",
      "Classe 4: 39.87%\n",
      "Classe 5: 35.68%\n",
      "Classe 6: 68.67%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 208/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3177, Acurácia: 0.4874\n",
      "Validação - Erro: 1.5262, Acurácia: 0.4427\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 22.70%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 25.40%\n",
      "Classe 3: 70.28%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 31.70%\n",
      "Classe 6: 61.93%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 209/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3166, Acurácia: 0.4922\n",
      "Validação - Erro: 1.5418, Acurácia: 0.4333\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 18.20%\n",
      "Classe 1: 12.50%\n",
      "Classe 2: 24.80%\n",
      "Classe 3: 67.93%\n",
      "Classe 4: 41.85%\n",
      "Classe 5: 30.78%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 210/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3061, Acurácia: 0.4931\n",
      "Validação - Erro: 1.5479, Acurácia: 0.4472\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.27%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 22.18%\n",
      "Classe 3: 71.62%\n",
      "Classe 4: 36.90%\n",
      "Classe 5: 34.46%\n",
      "Classe 6: 62.89%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 211/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3015, Acurácia: 0.4967\n",
      "Validação - Erro: 1.5771, Acurácia: 0.4411\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.84%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 21.37%\n",
      "Classe 3: 66.37%\n",
      "Classe 4: 42.17%\n",
      "Classe 5: 36.45%\n",
      "Classe 6: 66.27%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 212/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.3027, Acurácia: 0.4947\n",
      "Validação - Erro: 1.5629, Acurácia: 0.4472\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.77%\n",
      "Classe 1: 23.21%\n",
      "Classe 2: 27.42%\n",
      "Classe 3: 73.41%\n",
      "Classe 4: 35.58%\n",
      "Classe 5: 32.92%\n",
      "Classe 6: 65.30%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 213/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2834, Acurácia: 0.5019\n",
      "Validação - Erro: 1.5711, Acurácia: 0.4363\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.99%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 19.76%\n",
      "Classe 3: 71.40%\n",
      "Classe 4: 42.83%\n",
      "Classe 5: 32.77%\n",
      "Classe 6: 59.04%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 214/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2862, Acurácia: 0.5037\n",
      "Validação - Erro: 1.5731, Acurácia: 0.4436\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 22.70%\n",
      "Classe 1: 16.07%\n",
      "Classe 2: 22.38%\n",
      "Classe 3: 67.82%\n",
      "Classe 4: 48.76%\n",
      "Classe 5: 36.14%\n",
      "Classe 6: 54.70%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 215/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2807, Acurácia: 0.5053\n",
      "Validação - Erro: 1.5592, Acurácia: 0.4441\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 18.75%\n",
      "Classe 3: 66.03%\n",
      "Classe 4: 48.11%\n",
      "Classe 5: 31.55%\n",
      "Classe 6: 64.58%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 216/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2748, Acurácia: 0.5119\n",
      "Validação - Erro: 1.6031, Acurácia: 0.4372\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.41%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 13.10%\n",
      "Classe 3: 63.80%\n",
      "Classe 4: 50.25%\n",
      "Classe 5: 30.32%\n",
      "Classe 6: 69.88%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 217/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2614, Acurácia: 0.5133\n",
      "Validação - Erro: 1.5896, Acurácia: 0.4447\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.84%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 20.77%\n",
      "Classe 3: 69.72%\n",
      "Classe 4: 45.30%\n",
      "Classe 5: 34.61%\n",
      "Classe 6: 60.24%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 218/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2559, Acurácia: 0.5177\n",
      "Validação - Erro: 1.5999, Acurácia: 0.4422\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.98%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 26.81%\n",
      "Classe 3: 63.91%\n",
      "Classe 4: 43.99%\n",
      "Classe 5: 32.31%\n",
      "Classe 6: 64.10%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 219/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2532, Acurácia: 0.5171\n",
      "Validação - Erro: 1.5908, Acurácia: 0.4444\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 25.91%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 16.53%\n",
      "Classe 3: 72.07%\n",
      "Classe 4: 45.96%\n",
      "Classe 5: 27.26%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 220/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2484, Acurácia: 0.5202\n",
      "Validação - Erro: 1.6114, Acurácia: 0.4341\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.48%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 15.73%\n",
      "Classe 3: 64.80%\n",
      "Classe 4: 49.59%\n",
      "Classe 5: 27.57%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 221/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2365, Acurácia: 0.5250\n",
      "Validação - Erro: 1.6016, Acurácia: 0.4427\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.56%\n",
      "Classe 1: 26.79%\n",
      "Classe 2: 26.41%\n",
      "Classe 3: 71.62%\n",
      "Classe 4: 38.06%\n",
      "Classe 5: 35.53%\n",
      "Classe 6: 58.55%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 222/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2329, Acurácia: 0.5259\n",
      "Validação - Erro: 1.6450, Acurácia: 0.4436\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.12%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 19.76%\n",
      "Classe 3: 71.28%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 34.46%\n",
      "Classe 6: 57.35%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 223/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2256, Acurácia: 0.5293\n",
      "Validação - Erro: 1.6095, Acurácia: 0.4419\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.77%\n",
      "Classe 1: 26.79%\n",
      "Classe 2: 20.36%\n",
      "Classe 3: 66.82%\n",
      "Classe 4: 47.94%\n",
      "Classe 5: 30.63%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 224/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2174, Acurácia: 0.5341\n",
      "Validação - Erro: 1.6175, Acurácia: 0.4480\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 23.98%\n",
      "Classe 1: 26.79%\n",
      "Classe 2: 26.01%\n",
      "Classe 3: 68.04%\n",
      "Classe 4: 40.53%\n",
      "Classe 5: 30.63%\n",
      "Classe 6: 71.57%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 225/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.2076, Acurácia: 0.5390\n",
      "Validação - Erro: 1.6614, Acurácia: 0.4497\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.19%\n",
      "Classe 1: 21.43%\n",
      "Classe 2: 22.58%\n",
      "Classe 3: 71.06%\n",
      "Classe 4: 38.88%\n",
      "Classe 5: 31.24%\n",
      "Classe 6: 65.78%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 226/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1946, Acurácia: 0.5412\n",
      "Validação - Erro: 1.6093, Acurácia: 0.4436\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.41%\n",
      "Classe 1: 19.64%\n",
      "Classe 2: 25.40%\n",
      "Classe 3: 65.81%\n",
      "Classe 4: 43.16%\n",
      "Classe 5: 33.84%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 227/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:29<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1909, Acurácia: 0.5446\n",
      "Validação - Erro: 1.6799, Acurácia: 0.4447\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 22.06%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 25.00%\n",
      "Classe 3: 66.48%\n",
      "Classe 4: 46.95%\n",
      "Classe 5: 29.56%\n",
      "Classe 6: 67.23%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 228/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1917, Acurácia: 0.5421\n",
      "Validação - Erro: 1.6482, Acurácia: 0.4483\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 24.80%\n",
      "Classe 3: 67.71%\n",
      "Classe 4: 42.67%\n",
      "Classe 5: 31.24%\n",
      "Classe 6: 64.82%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 229/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1786, Acurácia: 0.5460\n",
      "Validação - Erro: 1.6986, Acurácia: 0.4422\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.77%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 26.01%\n",
      "Classe 3: 66.15%\n",
      "Classe 4: 38.22%\n",
      "Classe 5: 30.93%\n",
      "Classe 6: 70.60%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 230/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1705, Acurácia: 0.5512\n",
      "Validação - Erro: 1.6592, Acurácia: 0.4461\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.69%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 20.16%\n",
      "Classe 3: 68.16%\n",
      "Classe 4: 39.87%\n",
      "Classe 5: 33.84%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 231/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1660, Acurácia: 0.5519\n",
      "Validação - Erro: 1.6824, Acurácia: 0.4352\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 23.77%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 21.37%\n",
      "Classe 3: 67.26%\n",
      "Classe 4: 43.66%\n",
      "Classe 5: 27.26%\n",
      "Classe 6: 68.43%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 232/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1573, Acurácia: 0.5549\n",
      "Validação - Erro: 1.7433, Acurácia: 0.4466\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.99%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 26.61%\n",
      "Classe 3: 68.60%\n",
      "Classe 4: 42.01%\n",
      "Classe 5: 34.76%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 233/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1451, Acurácia: 0.5653\n",
      "Validação - Erro: 1.7900, Acurácia: 0.4341\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.34%\n",
      "Classe 1: 28.57%\n",
      "Classe 2: 25.81%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 42.17%\n",
      "Classe 5: 27.72%\n",
      "Classe 6: 65.30%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 234/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1336, Acurácia: 0.5648\n",
      "Validação - Erro: 1.6506, Acurácia: 0.4416\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.20%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 28.63%\n",
      "Classe 3: 67.60%\n",
      "Classe 4: 33.44%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 235/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1258, Acurácia: 0.5713\n",
      "Validação - Erro: 1.7275, Acurácia: 0.4517\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 24.84%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 28.23%\n",
      "Classe 3: 67.15%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 33.23%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 236/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1181, Acurácia: 0.5730\n",
      "Validação - Erro: 1.7954, Acurácia: 0.4347\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.77%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 24.60%\n",
      "Classe 3: 61.34%\n",
      "Classe 4: 49.26%\n",
      "Classe 5: 31.85%\n",
      "Classe 6: 57.83%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 237/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1196, Acurácia: 0.5731\n",
      "Validação - Erro: 1.7490, Acurácia: 0.4391\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 24.41%\n",
      "Classe 1: 30.36%\n",
      "Classe 2: 26.21%\n",
      "Classe 3: 63.46%\n",
      "Classe 4: 44.48%\n",
      "Classe 5: 28.48%\n",
      "Classe 6: 70.12%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 238/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.1003, Acurácia: 0.5828\n",
      "Validação - Erro: 1.8167, Acurácia: 0.4570\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 23.55%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 25.81%\n",
      "Classe 3: 69.72%\n",
      "Classe 4: 40.86%\n",
      "Classe 5: 36.75%\n",
      "Classe 6: 65.30%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 239/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0920, Acurácia: 0.5843\n",
      "Validação - Erro: 1.7708, Acurácia: 0.4405\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.77%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 23.99%\n",
      "Classe 3: 67.93%\n",
      "Classe 4: 42.50%\n",
      "Classe 5: 30.93%\n",
      "Classe 6: 60.00%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 240/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0839, Acurácia: 0.5880\n",
      "Validação - Erro: 1.8412, Acurácia: 0.4444\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.55%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 23.59%\n",
      "Classe 3: 67.71%\n",
      "Classe 4: 46.29%\n",
      "Classe 5: 25.42%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 241/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0693, Acurácia: 0.5923\n",
      "Validação - Erro: 1.7937, Acurácia: 0.4441\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 21.20%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 36.09%\n",
      "Classe 3: 63.13%\n",
      "Classe 4: 38.06%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 242/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0654, Acurácia: 0.5936\n",
      "Validação - Erro: 1.8090, Acurácia: 0.4514\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.27%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 27.82%\n",
      "Classe 3: 68.49%\n",
      "Classe 4: 37.56%\n",
      "Classe 5: 33.54%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 243/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0541, Acurácia: 0.5986\n",
      "Validação - Erro: 1.8241, Acurácia: 0.4386\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 24.60%\n",
      "Classe 3: 63.69%\n",
      "Classe 4: 39.54%\n",
      "Classe 5: 32.47%\n",
      "Classe 6: 67.71%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 244/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0486, Acurácia: 0.6033\n",
      "Validação - Erro: 1.8265, Acurácia: 0.4536\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 24.84%\n",
      "Classe 1: 25.00%\n",
      "Classe 2: 22.38%\n",
      "Classe 3: 67.60%\n",
      "Classe 4: 42.17%\n",
      "Classe 5: 41.35%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 245/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0304, Acurácia: 0.6120\n",
      "Validação - Erro: 1.8594, Acurácia: 0.4377\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 25.48%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 25.60%\n",
      "Classe 3: 65.70%\n",
      "Classe 4: 38.88%\n",
      "Classe 5: 31.39%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 246/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0228, Acurácia: 0.6132\n",
      "Validação - Erro: 1.8051, Acurácia: 0.4469\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.41%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 33.27%\n",
      "Classe 3: 58.55%\n",
      "Classe 4: 41.35%\n",
      "Classe 5: 36.45%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 247/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0157, Acurácia: 0.6147\n",
      "Validação - Erro: 1.9461, Acurácia: 0.4444\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.91%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 30.44%\n",
      "Classe 3: 63.46%\n",
      "Classe 4: 45.30%\n",
      "Classe 5: 32.01%\n",
      "Classe 6: 57.11%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 248/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 1.0014, Acurácia: 0.6222\n",
      "Validação - Erro: 1.9499, Acurácia: 0.4461\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 22.70%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 35.08%\n",
      "Classe 3: 67.71%\n",
      "Classe 4: 35.26%\n",
      "Classe 5: 34.30%\n",
      "Classe 6: 61.93%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 249/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9890, Acurácia: 0.6284\n",
      "Validação - Erro: 1.9489, Acurácia: 0.4514\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 29.55%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 30.65%\n",
      "Classe 3: 66.82%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 32.01%\n",
      "Classe 6: 60.24%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 250/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9852, Acurácia: 0.6276\n",
      "Validação - Erro: 1.9260, Acurácia: 0.4452\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 26.12%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 33.87%\n",
      "Classe 3: 64.47%\n",
      "Classe 4: 39.70%\n",
      "Classe 5: 29.71%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 251/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9743, Acurácia: 0.6324\n",
      "Validação - Erro: 1.9022, Acurácia: 0.4475\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.48%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 24.19%\n",
      "Classe 3: 62.23%\n",
      "Classe 4: 46.46%\n",
      "Classe 5: 33.69%\n",
      "Classe 6: 62.89%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 252/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9689, Acurácia: 0.6351\n",
      "Validação - Erro: 2.0285, Acurácia: 0.4464\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.69%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 29.44%\n",
      "Classe 3: 64.02%\n",
      "Classe 4: 38.06%\n",
      "Classe 5: 32.77%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 253/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9544, Acurácia: 0.6408\n",
      "Validação - Erro: 1.9348, Acurácia: 0.4531\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 23.77%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 29.44%\n",
      "Classe 3: 68.38%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 33.54%\n",
      "Classe 6: 65.54%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 254/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9461, Acurácia: 0.6448\n",
      "Validação - Erro: 1.9788, Acurácia: 0.4480\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 29.64%\n",
      "Classe 3: 66.15%\n",
      "Classe 4: 41.19%\n",
      "Classe 5: 34.92%\n",
      "Classe 6: 58.31%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 255/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9297, Acurácia: 0.6503\n",
      "Validação - Erro: 1.9730, Acurácia: 0.4542\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.41%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 66.37%\n",
      "Classe 4: 31.96%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 256/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9292, Acurácia: 0.6495\n",
      "Validação - Erro: 2.0700, Acurácia: 0.4500\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 24.20%\n",
      "Classe 1: 32.14%\n",
      "Classe 2: 27.82%\n",
      "Classe 3: 74.41%\n",
      "Classe 4: 39.04%\n",
      "Classe 5: 30.32%\n",
      "Classe 6: 59.04%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 257/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.9047, Acurácia: 0.6610\n",
      "Validação - Erro: 2.0545, Acurácia: 0.4492\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 29.34%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 31.85%\n",
      "Classe 3: 67.04%\n",
      "Classe 4: 38.88%\n",
      "Classe 5: 29.10%\n",
      "Classe 6: 65.30%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 258/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:44<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8958, Acurácia: 0.6677\n",
      "Validação - Erro: 2.1206, Acurácia: 0.4528\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 20.13%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 31.05%\n",
      "Classe 3: 65.03%\n",
      "Classe 4: 38.71%\n",
      "Classe 5: 41.35%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 259/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8934, Acurácia: 0.6647\n",
      "Validação - Erro: 2.1341, Acurácia: 0.4475\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 32.12%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 27.22%\n",
      "Classe 3: 68.27%\n",
      "Classe 4: 44.65%\n",
      "Classe 5: 28.64%\n",
      "Classe 6: 55.18%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 260/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8729, Acurácia: 0.6744\n",
      "Validação - Erro: 2.1345, Acurácia: 0.4505\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.05%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 32.26%\n",
      "Classe 3: 67.49%\n",
      "Classe 4: 39.21%\n",
      "Classe 5: 29.40%\n",
      "Classe 6: 65.54%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 261/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8712, Acurácia: 0.6772\n",
      "Validação - Erro: 2.1488, Acurácia: 0.4511\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.84%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 32.26%\n",
      "Classe 3: 62.79%\n",
      "Classe 4: 38.71%\n",
      "Classe 5: 35.83%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 262/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8624, Acurácia: 0.6780\n",
      "Validação - Erro: 2.1018, Acurácia: 0.4469\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.91%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 32.46%\n",
      "Classe 3: 62.57%\n",
      "Classe 4: 36.24%\n",
      "Classe 5: 34.61%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 263/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8520, Acurácia: 0.6829\n",
      "Validação - Erro: 2.1467, Acurácia: 0.4466\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.05%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 30.85%\n",
      "Classe 3: 64.58%\n",
      "Classe 4: 36.74%\n",
      "Classe 5: 34.46%\n",
      "Classe 6: 61.93%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 264/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8354, Acurácia: 0.6877\n",
      "Validação - Erro: 2.1750, Acurácia: 0.4575\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 32.55%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 32.06%\n",
      "Classe 3: 65.47%\n",
      "Classe 4: 34.27%\n",
      "Classe 5: 35.68%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 265/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:44<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8233, Acurácia: 0.6947\n",
      "Validação - Erro: 2.1425, Acurácia: 0.4542\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 33.06%\n",
      "Classe 3: 64.13%\n",
      "Classe 4: 36.24%\n",
      "Classe 5: 32.16%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 266/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8168, Acurácia: 0.6987\n",
      "Validação - Erro: 2.2141, Acurácia: 0.4525\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 24.41%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 30.44%\n",
      "Classe 3: 65.36%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 34.15%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 267/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.8101, Acurácia: 0.7002\n",
      "Validação - Erro: 2.1459, Acurácia: 0.4497\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 33.93%\n",
      "Classe 2: 34.88%\n",
      "Classe 3: 63.80%\n",
      "Classe 4: 35.58%\n",
      "Classe 5: 33.08%\n",
      "Classe 6: 62.41%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 268/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7945, Acurácia: 0.7067\n",
      "Validação - Erro: 2.1595, Acurácia: 0.4692\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 32.12%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 33.47%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 35.91%\n",
      "Classe 5: 40.43%\n",
      "Classe 6: 69.64%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "Melhor modelo salvo!\n",
      "\n",
      "Época 269/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7830, Acurácia: 0.7089\n",
      "Validação - Erro: 2.1868, Acurácia: 0.4653\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.05%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 33.67%\n",
      "Classe 3: 64.02%\n",
      "Classe 4: 43.16%\n",
      "Classe 5: 37.37%\n",
      "Classe 6: 65.30%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 270/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:44<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7831, Acurácia: 0.7103\n",
      "Validação - Erro: 2.1948, Acurácia: 0.4636\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.41%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 66.37%\n",
      "Classe 4: 37.40%\n",
      "Classe 5: 39.05%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 271/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7671, Acurácia: 0.7174\n",
      "Validação - Erro: 2.4131, Acurácia: 0.4556\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 34.69%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 31.45%\n",
      "Classe 3: 63.02%\n",
      "Classe 4: 43.49%\n",
      "Classe 5: 29.86%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 272/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7564, Acurácia: 0.7228\n",
      "Validação - Erro: 2.3030, Acurácia: 0.4514\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.84%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 32.66%\n",
      "Classe 3: 61.68%\n",
      "Classe 4: 40.20%\n",
      "Classe 5: 35.53%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 273/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7576, Acurácia: 0.7195\n",
      "Validação - Erro: 2.3036, Acurácia: 0.4575\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 33.40%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.87%\n",
      "Classe 3: 62.23%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 33.84%\n",
      "Classe 6: 63.86%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 274/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:43<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7339, Acurácia: 0.7307\n",
      "Validação - Erro: 2.3696, Acurácia: 0.4620\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 36.29%\n",
      "Classe 3: 61.79%\n",
      "Classe 4: 41.68%\n",
      "Classe 5: 30.93%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 275/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:48<00:00,  1.10it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7344, Acurácia: 0.7317\n",
      "Validação - Erro: 2.3435, Acurácia: 0.4597\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 35.71%\n",
      "Classe 2: 31.85%\n",
      "Classe 3: 67.37%\n",
      "Classe 4: 35.09%\n",
      "Classe 5: 37.52%\n",
      "Classe 6: 67.71%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 276/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:00<00:00,  1.07it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7175, Acurácia: 0.7343\n",
      "Validação - Erro: 2.3843, Acurácia: 0.4572\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.91%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 36.69%\n",
      "Classe 3: 61.12%\n",
      "Classe 4: 43.00%\n",
      "Classe 5: 33.38%\n",
      "Classe 6: 63.37%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 277/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:06<00:00,  1.05it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7044, Acurácia: 0.7406\n",
      "Validação - Erro: 2.3809, Acurácia: 0.4564\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 33.19%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 29.84%\n",
      "Classe 3: 62.79%\n",
      "Classe 4: 42.50%\n",
      "Classe 5: 32.92%\n",
      "Classe 6: 66.99%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 278/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:26<00:00,  1.01it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:33<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.7017, Acurácia: 0.7431\n",
      "Validação - Erro: 2.4408, Acurácia: 0.4642\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 27.84%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 35.89%\n",
      "Classe 3: 67.71%\n",
      "Classe 4: 39.21%\n",
      "Classe 5: 34.61%\n",
      "Classe 6: 63.61%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 279/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:30<00:00,  1.00s/it]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6894, Acurácia: 0.7466\n",
      "Validação - Erro: 2.3836, Acurácia: 0.4670\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.91%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 31.25%\n",
      "Classe 3: 66.48%\n",
      "Classe 4: 38.88%\n",
      "Classe 5: 37.67%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 280/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:56<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6807, Acurácia: 0.7494\n",
      "Validação - Erro: 2.4984, Acurácia: 0.4480\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 32.12%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 34.48%\n",
      "Classe 3: 58.77%\n",
      "Classe 4: 40.86%\n",
      "Classe 5: 32.31%\n",
      "Classe 6: 67.47%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 281/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:57<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6692, Acurácia: 0.7567\n",
      "Validação - Erro: 2.5303, Acurácia: 0.4439\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 28.05%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 29.44%\n",
      "Classe 3: 57.54%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 39.36%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 282/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:53<00:00,  1.09it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6677, Acurácia: 0.7577\n",
      "Validação - Erro: 2.4216, Acurácia: 0.4692\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 33.19%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 30.44%\n",
      "Classe 3: 67.82%\n",
      "Classe 4: 43.49%\n",
      "Classe 5: 33.38%\n",
      "Classe 6: 64.34%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 283/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:56<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6494, Acurácia: 0.7625\n",
      "Validação - Erro: 2.5316, Acurácia: 0.4645\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 29.98%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 39.11%\n",
      "Classe 3: 60.89%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 36.29%\n",
      "Classe 6: 67.95%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 284/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:56<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6417, Acurácia: 0.7643\n",
      "Validação - Erro: 2.5322, Acurácia: 0.4650\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.48%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 39.52%\n",
      "Classe 3: 63.24%\n",
      "Classe 4: 41.52%\n",
      "Classe 5: 34.61%\n",
      "Classe 6: 62.17%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 285/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:47<00:00,  1.10it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6439, Acurácia: 0.7649\n",
      "Validação - Erro: 2.5405, Acurácia: 0.4567\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.05%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 38.51%\n",
      "Classe 3: 66.26%\n",
      "Classe 4: 32.62%\n",
      "Classe 5: 33.08%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 286/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:01<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6303, Acurácia: 0.7715\n",
      "Validação - Erro: 2.5909, Acurácia: 0.4589\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.26%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 39.31%\n",
      "Classe 3: 61.23%\n",
      "Classe 4: 36.41%\n",
      "Classe 5: 38.44%\n",
      "Classe 6: 63.13%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 287/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:57<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6231, Acurácia: 0.7712\n",
      "Validação - Erro: 2.5114, Acurácia: 0.4583\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 32.12%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 35.69%\n",
      "Classe 3: 61.90%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 36.29%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 288/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:00<00:00,  1.07it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6220, Acurácia: 0.7721\n",
      "Validação - Erro: 2.5075, Acurácia: 0.4617\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 36.40%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 33.27%\n",
      "Classe 3: 61.01%\n",
      "Classe 4: 40.69%\n",
      "Classe 5: 35.83%\n",
      "Classe 6: 66.02%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 289/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:20<00:00,  1.02it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.6002, Acurácia: 0.7836\n",
      "Validação - Erro: 2.5852, Acurácia: 0.4553\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 33.83%\n",
      "Classe 1: 50.00%\n",
      "Classe 2: 37.90%\n",
      "Classe 3: 62.23%\n",
      "Classe 4: 36.41%\n",
      "Classe 5: 35.53%\n",
      "Classe 6: 60.24%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 290/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:06<00:00,  1.05it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:34<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5979, Acurácia: 0.7827\n",
      "Validação - Erro: 2.6519, Acurácia: 0.4667\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 37.26%\n",
      "Classe 1: 39.29%\n",
      "Classe 2: 36.09%\n",
      "Classe 3: 65.59%\n",
      "Classe 4: 33.94%\n",
      "Classe 5: 35.38%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 291/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:05<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5854, Acurácia: 0.7883\n",
      "Validação - Erro: 2.6181, Acurácia: 0.4650\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 33.62%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 33.67%\n",
      "Classe 3: 63.58%\n",
      "Classe 4: 38.88%\n",
      "Classe 5: 37.83%\n",
      "Classe 6: 64.82%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 292/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:04<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5785, Acurácia: 0.7905\n",
      "Validação - Erro: 2.7844, Acurácia: 0.4625\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 35.97%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 34.27%\n",
      "Classe 3: 67.26%\n",
      "Classe 4: 39.87%\n",
      "Classe 5: 30.32%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 293/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:03<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5775, Acurácia: 0.7868\n",
      "Validação - Erro: 2.6519, Acurácia: 0.4678\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 35.76%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 36.49%\n",
      "Classe 3: 64.13%\n",
      "Classe 4: 37.73%\n",
      "Classe 5: 37.52%\n",
      "Classe 6: 62.65%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 294/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:06<00:00,  1.05it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:32<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5642, Acurácia: 0.7965\n",
      "Validação - Erro: 2.7106, Acurácia: 0.4631\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 31.48%\n",
      "Classe 1: 42.86%\n",
      "Classe 2: 32.86%\n",
      "Classe 3: 65.14%\n",
      "Classe 4: 36.57%\n",
      "Classe 5: 36.45%\n",
      "Classe 6: 68.67%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 295/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:10<00:00,  1.04it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:34<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5642, Acurácia: 0.7945\n",
      "Validação - Erro: 2.6388, Acurácia: 0.4592\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 30.62%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 31.65%\n",
      "Classe 3: 61.12%\n",
      "Classe 4: 37.56%\n",
      "Classe 5: 40.43%\n",
      "Classe 6: 68.92%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 296/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:50<00:00,  1.09it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5547, Acurácia: 0.7961\n",
      "Validação - Erro: 2.7971, Acurácia: 0.4639\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 36.83%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 35.48%\n",
      "Classe 3: 58.88%\n",
      "Classe 4: 44.48%\n",
      "Classe 5: 33.69%\n",
      "Classe 6: 66.75%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 297/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:44<00:00,  1.11it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5422, Acurácia: 0.8013\n",
      "Validação - Erro: 2.7588, Acurácia: 0.4609\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 41.07%\n",
      "Classe 2: 33.06%\n",
      "Classe 3: 64.25%\n",
      "Classe 4: 37.89%\n",
      "Classe 5: 36.91%\n",
      "Classe 6: 61.69%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 298/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [06:54<00:00,  1.08it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:31<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5406, Acurácia: 0.8041\n",
      "Validação - Erro: 2.7450, Acurácia: 0.4561\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 34.26%\n",
      "Classe 1: 46.43%\n",
      "Classe 2: 40.73%\n",
      "Classe 3: 57.77%\n",
      "Classe 4: 37.07%\n",
      "Classe 5: 36.29%\n",
      "Classe 6: 65.06%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 299/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [07:04<00:00,  1.06it/s]\n",
      "Validando: 100%|██████████| 57/57 [00:33<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5285, Acurácia: 0.8095\n",
      "Validação - Erro: 2.7678, Acurácia: 0.4659\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 35.33%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 35.48%\n",
      "Classe 3: 62.46%\n",
      "Classe 4: 36.08%\n",
      "Classe 5: 39.20%\n",
      "Classe 6: 66.51%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n",
      "\n",
      "Época 300/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando: 100%|██████████| 449/449 [19:12<00:00,  2.57s/it]\n",
      "Validando: 100%|██████████| 57/57 [01:56<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino - Erro: 0.5193, Acurácia: 0.8105\n",
      "Validação - Erro: 2.7758, Acurácia: 0.4689\n",
      "Acurácia por classe na validação:\n",
      "Classe 0: 34.48%\n",
      "Classe 1: 37.50%\n",
      "Classe 2: 36.69%\n",
      "Classe 3: 64.69%\n",
      "Classe 4: 35.75%\n",
      "Classe 5: 36.75%\n",
      "Classe 6: 68.19%\n",
      "Checkpoint salvo: ./checkpoints\\ultimo_checkpoint_v3.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# Configurações gerais\n",
    "numero_de_epocas = 300\n",
    "bs = 64\n",
    "image_size = (96, 72)  # Atualizado para corresponder ao modelo customizado\n",
    "patches = (16, 16)  # Tamanho do patch do ViT\n",
    "num_classes = 7  # Atualize de acordo com seu dataset\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Criar diretório de checkpoints se não existir\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Transformações para as imagens\n",
    "transformacoes_de_imagens = {\n",
    "    'treino': transforms.Compose([\n",
    "        transforms.Resize(size=image_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "\n",
    "    'validacao': transforms.Compose([\n",
    "        transforms.Resize(size=image_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Carregar datasets\n",
    "dataset = './data/Fer-2013/'\n",
    "pasta_treino = os.path.join(dataset, 'treino')\n",
    "pasta_validacao = os.path.join(dataset, 'validacao')\n",
    "\n",
    "data = {\n",
    "    'treino': datasets.ImageFolder(root=pasta_treino, transform=transformacoes_de_imagens['treino']),\n",
    "    'validacao': datasets.ImageFolder(root=pasta_validacao, transform=transformacoes_de_imagens['validacao'])\n",
    "}\n",
    "\n",
    "# Criar DataLoaders\n",
    "data_loader_treino = DataLoader(data['treino'], batch_size=bs, shuffle=True, num_workers=4)\n",
    "data_loader_validacao = DataLoader(data['validacao'], batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "# Definir o modelo ViT customizado\n",
    " \n",
    "\n",
    "vit_model = ViT(\n",
    "    d_model=512,  # Dimensão do modelo\n",
    "    num_blks=12,  # Número de blocos do transformer\n",
    "    nhead=8,  # Número de cabeças de atenção\n",
    "    patches=patches,\n",
    "    img_size=image_size,\n",
    "    first_channel=3,  # Número de canais de entrada\n",
    "    dropout=0.1,\n",
    "    report_params_count=True\n",
    ")\n",
    "\n",
    "# Classificador para ajustar a saída do ViT\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax para compatibilidade com NLLLoss\n",
    ")\n",
    "\n",
    "vit_model.to(device)\n",
    "classifier.to(device)\n",
    "\n",
    "# Definir a função de erro e o otimizador\n",
    "funcao_erro = nn.NLLLoss()  # Negative Log Likelihood Loss\n",
    "otimizador = optim.Adam(\n",
    "    list(vit_model.parameters()) + list(classifier.parameters()), \n",
    "    lr=0.0001\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'ultimo_checkpoint_v3.pth')\n",
    "epoca_inicial = carregar_checkpoint_vit(vit_model, classifier, otimizador, checkpoint_path)\n",
    "\n",
    "# Atualizar a função `treinar_e_validar` para salvar sempre no mesmo arquivo o último checkpoint\n",
    "def treinar_e_validar(modelo, classifier, metrica_erro, otimizador_sgd, epocas=25, iniciar_epoca=0, melhor_acuracia=0.0):\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Inicializar GradScaler para Mixed Precision\n",
    "    historico = []\n",
    "\n",
    "    for epoca in range(iniciar_epoca, epocas):\n",
    "        inicio_epoca = time.time()\n",
    "        print(f\"\\nÉpoca {epoca + 1}/{epocas}\")\n",
    "\n",
    "        # Modo de treinamento\n",
    "        modelo.train()\n",
    "        classifier.train()\n",
    "        erro_treino = 0.0\n",
    "        acuracia_treino = 0.0\n",
    "\n",
    "        for entradas, labels in tqdm(data_loader_treino, desc=\"Treinando\"):\n",
    "            entradas, labels = entradas.to(device), labels.to(device)\n",
    "            otimizador_sgd.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():  # Mixed Precision\n",
    "                features = modelo(entradas)  # Extrair features do ViT\n",
    "                saidas = classifier(features)  # Passar pelo classificador\n",
    "                erro = metrica_erro(saidas, labels)  # Calcular perda\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(erro).backward()\n",
    "            scaler.step(otimizador_sgd)\n",
    "            scaler.update()\n",
    "\n",
    "            erro_treino += erro.item() * entradas.size(0)\n",
    "            _, preds = torch.max(saidas, 1)\n",
    "            acuracia_treino += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Modo de avaliação\n",
    "        modelo.eval()\n",
    "        classifier.eval()\n",
    "        erro_validacao = 0.0\n",
    "        acuracia_validacao = 0.0\n",
    "\n",
    "        # Inicializar variáveis para calcular a acurácia por classe\n",
    "        total_por_classe = torch.zeros(num_classes, device=device)\n",
    "        corretos_por_classe = torch.zeros(num_classes, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for entradas, labels in tqdm(data_loader_validacao, desc=\"Validando\"):\n",
    "                entradas, labels = entradas.to(device), labels.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    features = modelo(entradas)\n",
    "                    saidas = classifier(features)\n",
    "                    erro = metrica_erro(saidas, labels)\n",
    "\n",
    "                erro_validacao += erro.item() * entradas.size(0)\n",
    "                _, preds = torch.max(saidas, 1)\n",
    "                acuracia_validacao += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Atualizar contadores por classe\n",
    "                for classe in range(num_classes):\n",
    "                    total_por_classe[classe] += torch.sum(labels == classe)\n",
    "                    corretos_por_classe[classe] += torch.sum((preds == classe) & (labels == classe))\n",
    "\n",
    "        # Calcular métricas\n",
    "        erro_medio_treino = erro_treino / len(data['treino'])\n",
    "        acuracia_medio_treino = acuracia_treino.double() / len(data['treino'])\n",
    "        erro_medio_validacao = erro_validacao / len(data['validacao'])\n",
    "        acuracia_medio_validacao = acuracia_validacao.double() / len(data['validacao'])\n",
    "\n",
    "        historico.append([erro_medio_treino, erro_medio_validacao, acuracia_medio_treino, acuracia_medio_validacao])\n",
    "\n",
    "        print(f\"Treino - Erro: {erro_medio_treino:.4f}, Acurácia: {acuracia_medio_treino:.4f}\")\n",
    "        print(f\"Validação - Erro: {erro_medio_validacao:.4f}, Acurácia: {acuracia_medio_validacao:.4f}\")\n",
    "\n",
    "        # Mostrar acurácia por classe\n",
    "        print(\"Acurácia por classe na validação:\")\n",
    "        for classe in range(num_classes):\n",
    "            taxa_acerto = (corretos_por_classe[classe] / total_por_classe[classe]).item() if total_por_classe[classe] > 0 else 0.0\n",
    "            print(f\"Classe {classe}: {taxa_acerto * 100:.2f}%\")\n",
    "\n",
    "        # Salvar checkpoints\n",
    "        salvar_checkpoint_vit(modelo, classifier, otimizador_sgd, epoca + 1, checkpoint_path)\n",
    "\n",
    "        # Atualizar o melhor modelo\n",
    "        if acuracia_medio_validacao > melhor_acuracia:\n",
    "            melhor_acuracia = acuracia_medio_validacao\n",
    "            torch.save(modelo.state_dict(), 'melhor_modelo_vit_V3.pth')\n",
    "            torch.save(classifier.state_dict(), 'melhor_classifier_V3.pth')\n",
    "            print(\"Melhor modelo salvo!\")\n",
    "\n",
    "    return historico\n",
    "\n",
    "\n",
    "# Treinar o modelo\n",
    "historico = treinar_e_validar(vit_model, classifier, funcao_erro, otimizador, numero_de_epocas, iniciar_epoca=epoca_inicial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
